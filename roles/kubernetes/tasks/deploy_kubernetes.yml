---
- name: Check if Kubernetes deployment is enabled
  fail:
    msg: "Kubernetes deployment is disabled in user_input.yml"
  when: not kubernetes_deployment.enabled | default(false)

- name: Display current working directory
  command: pwd
  register: current_dir
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Read user_input.yml
  slurp:
    src: "{{ playbook_dir }}/user_input.yml"
  register: user_input_content
  when: kubernetes_deployment.enabled | default(false)

- name: Parse user_input.yml content
  set_fact:
    user_input: "{{ user_input_content.content | b64decode | from_yaml }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set Kubespray variables
  set_fact:
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
    loadbalancer_apiserver:
      address: "{{ kubernetes_deployment.api_server.host }}"
      port: "{{ kubernetes_deployment.api_server.port }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Generate inventory content
  template:
    src: "{{ playbook_dir }}/templates/inventory.ini.j2"
    dest: "{{ inventory_dir }}/kubespray/inventory.ini"
  vars:
    control_plane_nodes: "{{ kubernetes_deployment.control_plane_nodes }}"
    worker_nodes: "{{ kubernetes_deployment.worker_nodes | default([]) }}"
    ssh_key_path: "{{ kubernetes_deployment.ssh_key_path | default('~/.ssh/k8s_rsa') }}"
    default_ansible_user: "{{ kubernetes_deployment.default_ansible_user | default('root') }}"
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Show generated inventory file contents
  command: cat {{ inventory_dir }}/kubespray/inventory.ini
  register: inventory_contents
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Display debug information
  debug:
    msg: |
      Current Directory: {{ current_dir.stdout }}
      Inventory Path: {{ inventory_dir }}/kubespray/inventory.ini
      Kubespray Dir: {{ kubespray_dir }}
      Group Vars Path: {{ inventory_dir }}/kubespray/group_vars/all/all.yml
  when: kubernetes_deployment.enabled | default(false)

- name: Test SSH connection to all hosts
  command:
    cmd: "ssh -i {{ kubernetes_deployment.ssh_key_path | expanduser | realpath }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ item.ansible_user | default('root') }}@{{ item.ansible_host }} 'echo SSH connection successful'"
  loop: "{{ kubernetes_deployment.control_plane_nodes + kubernetes_deployment.worker_nodes | default([]) }}"
  register: ssh_test
  changed_when: false
  ignore_errors: true
  when: kubernetes_deployment.enabled | default(false)

- name: Display SSH test results
  debug:
    var: ssh_test
  when: kubernetes_deployment.enabled | default(false)

- name: Set Ansible environment variables
  set_fact:
    ansible_env_vars: "{{ kubespray_env | combine(kubespray_extra_env) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set default values for async execution
  set_fact:
    kubespray_async_timeout: "{{ kubernetes_deployment.async_config.timeout | default(3600) }}"
    kubespray_poll_interval: "{{ kubernetes_deployment.async_config.poll_interval | default(10) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Run Kubespray cluster deployment
  command:
    cmd: >
      ansible-playbook -i {{ inventory_dir }}/kubespray/inventory.ini
      {{ kubespray_dir }}/cluster.yml
      -e @{{ inventory_dir }}/kubespray/group_vars/all/all.yml
      -e @{{ inventory_dir }}/kubespray/group_vars/all/api_server.yml
      -e @{{ inventory_dir }}/kubespray/group_vars/k8s_cluster/addons.yml
      -e "kube_apiserver_ip={{ kubernetes_deployment.api_server.host }}"
      -e "loadbalancer_apiserver={address: '{{ kubernetes_deployment.api_server.host }}', port: {{ kubernetes_deployment.api_server.port }}}"
      -e "apiserver_loadbalancer_domain_name={{ kubernetes_deployment.api_server.host }}"
      -e "supplementary_addresses_in_ssl_keys=['{{ kubernetes_deployment.api_server.host }}']"
      -e "kubernetes_deployment={{ kubernetes_deployment }}"
      --become
      --become-method={{ kubespray_become_method }}
      --become-user={{ kubespray_become_user }}
      -e ansible_ssh_private_key_file={{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}
      -e ansible_user={{ kubernetes_deployment.default_ansible_user | default('root') }}
      -e ansible_become_pass=''
      --ssh-extra-args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
      -vvvv
  environment: "{{ kubespray_env | combine(kubespray_extra_env) | combine({'ANSIBLE_CONFIG': playbook_dir + '/ansible.cfg'}) }}"
  register: kubespray_result
  async: "{{ kubespray_async_timeout }}"
  poll: "{{ kubespray_poll_interval }}"
  ignore_errors: true
  when: kubernetes_deployment.enabled | default(false)

- name: Display warning if async timeout occurred
  debug:
    msg: "WARNING: Kubespray deployment exceeded the async timeout of {{ kubespray_async_timeout }} seconds. The deployment is still running in the background."
  when: 
    - kubespray_result is defined 
    - kubespray_result.msg is defined
    - kubespray_result.msg is search("async task did not complete within the requested time")

- name: Continue with deployment
  debug:
    msg: "Proceeding with the rest of the deployment tasks..."
  when: kubernetes_deployment.enabled | default(false)

- name: Display Kubespray deployment result
  debug:
    msg: |
      STDOUT:
      {{ kubespray_result.stdout_lines | default([]) | join('\n') }}
      
      STDERR:
      {{ kubespray_result.stderr_lines | default([]) | join('\n') }}
  when: kubespray_result is defined and kubernetes_deployment.enabled | default(false)

- name: Get current user
  command: whoami
  register: current_user
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get current user's group
  command: id -gn
  register: current_group
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get real home directory
  command: "echo $HOME"
  register: real_home
  changed_when: false
  become: false
  delegate_to: localhost

- name: Ensure files directory exists
  file:
    path: "{{ lookup('env', 'PWD') }}/files"
    state: directory
    mode: '0755'
  become: false
  when: kubernetes_deployment.enabled | default(false)

- name: Fix files directory ownership
  block:
    - name: Try fixing directory ownership without sudo
      file:
        path: "{{ lookup('env', 'PWD') }}/files"
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
        recurse: yes
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: dir_ownership_result

    - name: Fix directory ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ lookup('env', 'PWD') }}/files
        sudo chmod -R 755 {{ lookup('env', 'PWD') }}/files
      delegate_to: localhost
      ignore_errors: true
      when: dir_ownership_result is failed

- name: Create temporary directory on remote host
  file:
    path: "/tmp/kubeconfig-temp"
    state: directory
    mode: '0755'
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true

- name: Wait for kubeconfig file to be created
  wait_for:
    path: /etc/kubernetes/admin.conf
    state: present
    timeout: 300
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  register: kubeconfig_wait
  ignore_errors: true

- name: Copy kubeconfig to temporary location with sudo
  shell: "cp /etc/kubernetes/admin.conf /tmp/kubeconfig-temp/admin.conf && chmod 644 /tmp/kubeconfig-temp/admin.conf && chown {{ kubernetes_deployment.default_ansible_user }}:{{ kubernetes_deployment.default_ansible_user }} /tmp/kubeconfig-temp/admin.conf"
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  ignore_errors: true
  register: kubeconfig_copy_result
  when: kubeconfig_wait is success

- name: Display warning if kubeconfig copy failed
  debug:
    msg: "WARNING: Failed to copy kubeconfig from /etc/kubernetes/admin.conf. This is expected if the cluster is not yet fully initialized."
  when: kubeconfig_copy_result is failed

- name: Fetch kubeconfig from temporary location
  fetch:
    src: "/tmp/kubeconfig-temp/admin.conf"
    dest: "{{ lookup('env', 'PWD') }}/files/kubeconfig"
    flat: yes
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
  register: kubeconfig_fetch

- name: Clean up temporary directory
  file:
    path: "/tmp/kubeconfig-temp"
    state: absent
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  when: kubeconfig_fetch is success

- name: Fix all local file permissions and ownership
  block:
    - name: Try fixing permissions and ownership without sudo
      file:
        path: "{{ item }}"
        mode: '0644'  # Changed to allow read access for others
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
      with_items:
        - "{{ lookup('env', 'PWD') }}/files/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: fix_perms_result

    - name: Fix permissions and ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ item }}
        sudo chmod 644 {{ item }}  # Explicit permission setting
      with_items:
        - "{{ lookup('env', 'PWD') }}/files/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
      when: fix_perms_result is failed

    - name: Ensure directories have execute permission
      shell: |
        sudo chmod 755 {{ item }}
      with_items:
        - "{{ lookup('env', 'PWD') }}/files"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
  when: kubeconfig_fetch is success

- name: Verify kubeconfig exists
  stat:
    path: "{{ lookup('env', 'PWD') }}/files/kubeconfig"
  register: kubeconfig_stat
  delegate_to: localhost

- name: Display kubeconfig location
  debug:
    msg: |
      Kubeconfig has been copied to:
      - Global location: {{ lookup('env', 'PWD') }}/files/kubeconfig
      
      You can now use kubectl with:
      export KUBECONFIG={{ lookup('env', 'PWD') }}/files/kubeconfig
      kubectl get nodes
  when: kubeconfig_stat.stat.exists 
