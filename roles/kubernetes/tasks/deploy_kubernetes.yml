---
- name: Check if Kubernetes deployment is enabled
  fail:
    msg: "Kubernetes deployment is disabled in user_input.yml"
  when: not kubernetes_deployment.enabled | default(false)

- name: Initialize summary tracking
  include_tasks: "{{ playbook_dir }}/tasks/summary_tracker.yml"
  when: kubernetes_deployment.enabled | default(false)

- name: Display current working directory
  command: pwd
  register: current_dir
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Read user_input.yml
  slurp:
    src: "{{ playbook_dir }}/user_input.yml"
  register: user_input_content
  when: kubernetes_deployment.enabled | default(false)

- name: Parse user_input.yml content
  set_fact:
    user_input: "{{ user_input_content.content | b64decode | from_yaml }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set Kubespray variables
  set_fact:
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
    loadbalancer_apiserver:
      address: "{{ kubernetes_deployment.api_server.host }}"
      port: "{{ kubernetes_deployment.api_server.port }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Generate inventory content
  template:
    src: "{{ playbook_dir }}/templates/inventory.ini.j2"
    dest: "{{ inventory_dir }}/kubespray/inventory.ini"
  vars:
    control_plane_nodes: "{{ kubernetes_deployment.control_plane_nodes }}"
    worker_nodes: "{{ kubernetes_deployment.worker_nodes | default([]) }}"
    ssh_key_path: "{{ kubernetes_deployment.ssh_key_path | default('~/.ssh/k8s_rsa') }}"
    default_ansible_user: "{{ kubernetes_deployment.default_ansible_user | default('root') }}"
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Show generated inventory file contents
  command: cat {{ inventory_dir }}/kubespray/inventory.ini
  register: inventory_contents
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Display debug information
  debug:
    msg: |
      Current Directory: {{ current_dir.stdout }}
      Inventory Path: {{ inventory_dir }}/kubespray/inventory.ini
      Kubespray Dir: {{ kubespray_dir }}
      Group Vars Path: {{ inventory_dir }}/kubespray/group_vars/all/all.yml
  when: kubernetes_deployment.enabled | default(false)

- name: Test SSH connection to all hosts
  command:
    cmd: "ssh -i {{ kubernetes_deployment.ssh_key_path | expanduser | realpath }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ item.ansible_user | default('root') }}@{{ item.ansible_host }} 'echo SSH connection successful'"
  loop: "{{ kubernetes_deployment.control_plane_nodes + kubernetes_deployment.worker_nodes | default([]) }}"
  register: ssh_test
  changed_when: false
  ignore_errors: true
  when: kubernetes_deployment.enabled | default(false)

- name: Display SSH test results
  debug:
    var: ssh_test
  when: kubernetes_deployment.enabled | default(false)

- name: Set Ansible environment variables
  set_fact:
    ansible_env_vars: "{{ kubespray_env | combine(kubespray_extra_env) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set default values for async execution
  set_fact:
    kubespray_async_timeout: "{{ kubernetes_deployment.async_config.timeout | default(3600) }}"
    kubespray_poll_interval: "{{ kubernetes_deployment.async_config.poll_interval | default(10) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Clear Ansible fact cache to prevent IP conflicts
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - "~/.ansible/tmp"
    - "~/.ansible/cp"
    - "/tmp/ansible-facts-*"
  ignore_errors: true
  delegate_to: localhost
  when: kubernetes_deployment.enabled | default(false)

- name: Force refresh network facts on target hosts
  setup:
    gather_subset: '!all,network'
    filter: "ansible_default_ipv4"
  delegate_to: "{{ item.ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ item.ansible_user | default(kubernetes_deployment.default_ansible_user) }}"
  loop: "{{ kubernetes_deployment.control_plane_nodes + kubernetes_deployment.worker_nodes | default([]) }}"
  ignore_errors: true
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars directory for explicit IP configuration
  file:
    path: "{{ inventory_dir }}/kubespray/host_vars"
    state: directory
    mode: '0755'
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars files for control plane nodes with explicit IP settings
  template:
    src: "{{ playbook_dir }}/templates/host_vars.yml.j2"
    dest: "{{ inventory_dir }}/kubespray/host_vars/{{ item.name }}.yml"
  vars:
    node_private_ip: "{{ item.private_ip }}"
    node_access_ip: "{{ item.private_ip }}"
  loop: "{{ kubernetes_deployment.control_plane_nodes }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars files for worker nodes with explicit IP settings
  template:
    src: "{{ playbook_dir }}/templates/host_vars.yml.j2"
    dest: "{{ inventory_dir }}/kubespray/host_vars/{{ item.name }}.yml"
  vars:
    node_private_ip: "{{ item.private_ip }}"
    node_access_ip: "{{ item.private_ip }}"
  loop: "{{ kubernetes_deployment.worker_nodes | default([]) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Verify IP configuration before deployment
  debug:
    msg: |
      === IP Configuration Verification ===
      Control Plane Nodes:
      {% for node in kubernetes_deployment.control_plane_nodes %}
      - {{ node.name }}: 
        - ansible_host (public): {{ node.ansible_host }}
        - private_ip (etcd only): {{ node.private_ip }}
        - host_vars file: {{ inventory_dir }}/kubespray/host_vars/{{ node.name }}.yml
      {% endfor %}
      {% if kubernetes_deployment.worker_nodes is defined and kubernetes_deployment.worker_nodes|length > 0 %}
      Worker Nodes:
      {% for node in kubernetes_deployment.worker_nodes %}
      - {{ node.name }}:
        - ansible_host (public): {{ node.ansible_host }}
        - private_ip (etcd only): {{ node.private_ip }}
        - host_vars file: {{ inventory_dir }}/kubespray/host_vars/{{ node.name }}.yml
      {% endfor %}
      {% endif %}
      
      IMPORTANT: Only etcd-specific variables are overridden to fix binding issues.
      API Server will remain accessible on public IP: {{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}
  when: kubernetes_deployment.enabled | default(false)

- name: Set node IP variables
  set_fact:
    control_plane_ips: >-
      {{ kubernetes_deployment.control_plane_nodes | map(attribute='private_ip') | 
         map('default', ansible_default_ipv4.address) | list }}
    worker_node_ips: >-
      {{ kubernetes_deployment.worker_nodes | default([]) | map(attribute='private_ip') | 
         map('default', ansible_default_ipv4.address) | list }}
    first_control_plane_ip: >-
      {{ kubernetes_deployment.control_plane_nodes[0].private_ip | 
         default(hostvars[groups['kube_control_plane'][0]].ansible_default_ipv4.address) }}
  when: kubernetes_deployment.enabled | default(false)

- name: Track Kubernetes deployment start
  set_fact:
    installation_summary:
      total_items: "{{ (installation_summary.total_items | int) + 1 }}"
      successful_items: "{{ installation_summary.successful_items + [deployment_info] }}"
      failed_items: "{{ installation_summary.failed_items }}"
      skipped_items: "{{ installation_summary.skipped_items }}"
      kubernetes_components: "{{ installation_summary.kubernetes_components + [deployment_info] }}"
      helm_charts: "{{ installation_summary.helm_charts }}"
      manifests: "{{ installation_summary.manifests }}"
      kubectl_commands: "{{ installation_summary.kubectl_commands }}"
      command_executions: "{{ installation_summary.command_executions }}"
  vars:
    deployment_info:
      name: "kubespray-cluster-deployment"
      type: "kubernetes"
      status: "success"
      timestamp: "{{ ansible_date_time.iso8601 }}"
      details: "Starting Kubespray cluster deployment with {{ kubernetes_deployment.control_plane_nodes | length }} control plane nodes"
  when: 
    - summary_enabled | default(true)
    - kubernetes_deployment.enabled | default(false)

- name: Create Kubespray vars file
  template:
    src: kubespray_vars.yml.j2
    dest: /tmp/kubespray-vars.yml
  delegate_to: localhost

- name: Create extra vars file
  copy:
    content: |
      {
        "ansible_ssh_private_key_file": "{{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}",
        "ansible_user": "{{ kubernetes_deployment.default_ansible_user }}",
        "ansible_become_pass": "{{ kubernetes_deployment.ansible_sudo_pass }}",
        "ansible_python_interpreter": "/usr/bin/python3"
      }
    dest: /tmp/kubespray-extra-vars.json
  delegate_to: localhost

- name: Run Kubespray cluster deployment
  shell:
    cmd: >-
      ansible-playbook -i {{ inventory_dir }}/kubespray/inventory.ini cluster.yml
      -e @/tmp/kubespray-vars.yml
      -e @/tmp/kubespray-extra-vars.json
      -e ansible_ssh_private_key_file={{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}
      -e ansible_user={{ kubernetes_deployment.default_ansible_user | default('root') }}
      -e ansible_become_pass="{{ kubernetes_deployment.ansible_sudo_pass | default('') }}"
      -e ansible_async_dir=/tmp/ansible-async
      -e ANSIBLE_KEEP_REMOTE_FILES=1
      -e ANSIBLE_LOG_PATH={{ playbook_dir }}/ansible-kubespray.log
      --become
      --become-method=sudo
      --become-user=root
      --ssh-extra-args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
      --forks={{ kubernetes_deployment.async_config.forks | default(20) }}
      -vvvv
    chdir: "{{ kubespray_dir }}"
  environment:
    ANSIBLE_STDOUT_CALLBACK: "debug"
    ANSIBLE_SHOW_CUSTOM_STATS: "true"
    ANSIBLE_DISPLAY_SKIPPED_HOSTS: "true"
    ANSIBLE_DISPLAY_OK_HOSTS: "true"
    ANSIBLE_HOST_KEY_CHECKING: "false"
    ANSIBLE_RETRY_FILES_ENABLED: "true"
    ANSIBLE_RETRY_FILES_SAVE_PATH: "{{ playbook_dir }}/ansible-retry"
    ANSIBLE_INVENTORY_CACHE: "true"
    ANSIBLE_CACHE_PLUGIN: "jsonfile"
    ANSIBLE_CACHE_PLUGIN_CONNECTION: "{{ playbook_dir }}/ansible-cache"
    ANSIBLE_CACHE_PLUGIN_TIMEOUT: "86400"
    LC_ALL: "C.UTF-8"
    LANG: "C.UTF-8"
  async: "{{ kubernetes_deployment.async_config.timeout | default(3600) }}"
  poll: "{{ kubernetes_deployment.async_config.poll_interval | default(5) }}"
  register: kubespray_deployment
  delegate_to: localhost

- name: Wait for Kubespray deployment
  ansible.builtin.async_status:
    jid: "{{ kubespray_deployment.ansible_job_id }}"
  register: job_result
  until: job_result.finished
  retries: "{{ (kubernetes_deployment.async_config.timeout | default(3600)) | int // (kubernetes_deployment.async_config.poll_interval | default(5)) | int }}"
  delay: "{{ kubernetes_deployment.async_config.poll_interval | default(5) }}"
  when: kubespray_deployment.ansible_job_id is defined
  delegate_to: localhost

- name: Display deployment progress
  ansible.builtin.debug:
    msg: "Deployment Progress: {{ (job_result.elapsed | default(0) / kubernetes_deployment.async_config.timeout | default(3600) * 100) | round(2) }}% complete"
  when: job_result.finished is defined and not job_result.finished

- name: Check deployment result
  ansible.builtin.fail:
    msg: |
      Kubespray deployment failed. Check {{ playbook_dir }}/ansible-kubespray.log for details.
      Last error: {{ job_result.msg | default('Unknown error') }}
  when: job_result.rc is defined and job_result.rc != 0

- name: Track successful Kubernetes deployment
  set_fact:
    installation_summary:
      total_items: "{{ (installation_summary.total_items | int) + 1 }}"
      successful_items: "{{ installation_summary.successful_items + [deployment_info] }}"
      failed_items: "{{ installation_summary.failed_items }}"
      skipped_items: "{{ installation_summary.skipped_items }}"
      kubernetes_components: "{{ installation_summary.kubernetes_components + [deployment_info] }}"
      helm_charts: "{{ installation_summary.helm_charts }}"
      manifests: "{{ installation_summary.manifests }}"
      kubectl_commands: "{{ installation_summary.kubectl_commands }}"
      command_executions: "{{ installation_summary.command_executions }}"
  vars:
    deployment_info:
      name: "kubespray-cluster-deployment"
      type: "kubernetes"
      status: "success"
      timestamp: "{{ ansible_date_time.iso8601 }}"
      details: "Kubespray deployment completed successfully in {{ kubespray_deployment.elapsed | default('unknown time') }}"
  when: 
    - summary_enabled | default(true)
    - kubernetes_deployment.enabled | default(false)
    - kubespray_deployment is succeeded

- name: Track failed Kubernetes deployment
  set_fact:
    installation_summary:
      total_items: "{{ (installation_summary.total_items | int) + 1 }}"
      successful_items: "{{ installation_summary.successful_items }}"
      failed_items: "{{ installation_summary.failed_items + [deployment_info] }}"
      skipped_items: "{{ installation_summary.skipped_items }}"
      kubernetes_components: "{{ installation_summary.kubernetes_components }}"
      helm_charts: "{{ installation_summary.helm_charts }}"
      manifests: "{{ installation_summary.manifests }}"
      kubectl_commands: "{{ installation_summary.kubectl_commands }}"
      command_executions: "{{ installation_summary.command_executions }}"
  vars:
    deployment_info:
      name: "kubespray-cluster-deployment"
      type: "kubernetes"
      status: "failed"
      timestamp: "{{ ansible_date_time.iso8601 }}"
      error: "{{ kubespray_deployment.msg | default('Kubespray deployment failed') }} - RC: {{ kubespray_deployment.rc | default('unknown') }}"
      details: "Failed after {{ kubespray_deployment.elapsed | default('unknown time') }}"
  when: 
    - summary_enabled | default(true)
    - kubernetes_deployment.enabled | default(false)
    - kubespray_deployment is failed

- name: Display warning if async timeout occurred
  debug:
    msg: "WARNING: Kubespray deployment exceeded the async timeout of {{ kubernetes_deployment.async_config.timeout | default(3600) }} seconds. The deployment is still running in the background."
  when: 
    - kubespray_deployment is defined 
    - kubespray_deployment.msg is defined
    - kubespray_deployment.msg is search("async task did not complete within the requested time")

- name: Continue with deployment
  debug:
    msg: "Proceeding with the rest of the deployment tasks..."
  when: kubernetes_deployment.enabled | default(false)

- name: Display Kubespray deployment result
  debug:
    msg: |
      STDOUT:
      {{ kubespray_deployment.stdout_lines | default([]) | join('\n') }}
      
      STDERR:
      {{ kubespray_deployment.stderr_lines | default([]) | join('\n') }}
  when: kubespray_deployment is defined and kubernetes_deployment.enabled | default(false)

- name: Get current user
  command: whoami
  register: current_user
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get current user's group
  command: id -gn
  register: current_group
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get real home directory
  command: "echo $HOME"
  register: real_home
  changed_when: false
  become: false
  delegate_to: localhost

- name: Ensure output directory exists
  file:
    path: "{{ lookup('env', 'PWD') }}/output"
    state: directory
    mode: '0755'
  become: false
  when: kubernetes_deployment.enabled | default(false)

- name: Fix output directory ownership
  block:
    - name: Try fixing directory ownership without sudo
      file:
        path: "{{ lookup('env', 'PWD') }}/output"
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
        recurse: yes
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: dir_ownership_result

    - name: Fix directory ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ lookup('env', 'PWD') }}/output
        sudo chmod -R 755 {{ lookup('env', 'PWD') }}/output
      delegate_to: localhost
      ignore_errors: true
      when: dir_ownership_result is failed

- name: Create temporary directory on remote host
  file:
    path: "/tmp/kubeconfig-temp"
    state: directory
    mode: '0755'
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true

- name: Wait for kubeconfig file to be created
  wait_for:
    path: /etc/kubernetes/admin.conf
    state: present
    timeout: 600
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  register: kubeconfig_wait
  ignore_errors: true

- name: Copy kubeconfig to temporary location with sudo
  shell: "cp /etc/kubernetes/admin.conf /tmp/kubeconfig-temp/admin.conf && chmod 644 /tmp/kubeconfig-temp/admin.conf && chown {{ kubernetes_deployment.default_ansible_user }}:{{ kubernetes_deployment.default_ansible_user }} /tmp/kubeconfig-temp/admin.conf"
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  ignore_errors: true
  register: kubeconfig_copy_result
  when: kubeconfig_wait is success

- name: Display warning if kubeconfig copy failed
  debug:
    msg: "WARNING: Failed to copy kubeconfig from /etc/kubernetes/admin.conf. This is expected if the cluster is not yet fully initialized."
  when: kubeconfig_copy_result is failed or kubeconfig_wait is failed

- name: Fetch kubeconfig from temporary location
  fetch:
    src: "/tmp/kubeconfig-temp/admin.conf"
    dest: "{{ lookup('env', 'PWD') }}/output/kubeconfig"
    flat: yes
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
  register: kubeconfig_fetch
  when: kubeconfig_copy_result is success

- name: Clean up temporary directory
  file:
    path: "/tmp/kubeconfig-temp"
    state: absent
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  when: kubeconfig_fetch is success

- name: Fix all local file permissions and ownership
  block:
    - name: Try fixing permissions and ownership without sudo
      file:
        path: "{{ item }}"
        mode: '0644'  # Changed to allow read access for others
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
      with_items:
        - "{{ lookup('env', 'PWD') }}/output/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: fix_perms_result

    - name: Fix permissions and ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ item }}
        sudo chmod 644 {{ item }}  # Explicit permission setting
      with_items:
        - "{{ lookup('env', 'PWD') }}/output/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
      when: fix_perms_result is failed

    - name: Ensure directories have execute permission
      shell: |
        sudo chmod 755 {{ item }}
      with_items:
        - "{{ lookup('env', 'PWD') }}/output"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
  when: kubeconfig_fetch is success

- name: Verify kubeconfig exists
  stat:
    path: "{{ lookup('env', 'PWD') }}/output/kubeconfig"
  register: kubeconfig_stat
  delegate_to: localhost

- name: Display kubeconfig location
  debug:
    msg: |
      Kubeconfig has been copied to:
      - Global location: {{ lookup('env', 'PWD') }}/output/kubeconfig
      
      You can now use kubectl with:
      export KUBECONFIG={{ lookup('env', 'PWD') }}/output/kubeconfig
      kubectl get nodes
  when: kubeconfig_stat.stat.exists 