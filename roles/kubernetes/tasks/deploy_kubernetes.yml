---
- name: Check if Kubernetes deployment is enabled
  fail:
    msg: "Kubernetes deployment is disabled in user_input.yml"
  when: not kubernetes_deployment.enabled | default(false)

- name: Initialize summary tracking
  include_tasks: "{{ playbook_dir }}/tasks/summary_tracker.yml"
  when: kubernetes_deployment.enabled | default(false)

- name: Display current working directory
  command: pwd
  register: current_dir
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Read user_input.yml
  slurp:
    src: "{{ playbook_dir }}/user_input.yml"
  register: user_input_content
  delegate_to: localhost
  when: kubernetes_deployment.enabled | default(false)

- name: Parse user_input.yml content
  set_fact:
    user_input: "{{ user_input_content.content | b64decode | from_yaml }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set Kubespray variables
  set_fact:
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
    loadbalancer_apiserver:
      address: "{{ kubernetes_deployment.api_server.host }}"
      port: "{{ kubernetes_deployment.api_server.port }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Generate inventory content
  template:
    src: "{{ playbook_dir }}/templates/inventory.ini.j2"
    dest: "{{ inventory_dir }}/kubespray/inventory.ini"
  vars:
    control_plane_nodes: "{{ kubernetes_deployment.control_plane_nodes }}"
    worker_nodes: "{{ kubernetes_deployment.worker_nodes | default([]) }}"
    ssh_key_path: "{{ kubernetes_deployment.ssh_key_path | default('~/.ssh/k8s_rsa') }}"
    default_ansible_user: "{{ kubernetes_deployment.default_ansible_user | default('root') }}"
    kube_apiserver_ip: "{{ kubernetes_deployment.api_server.host }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Show generated inventory file contents
  command: cat {{ inventory_dir }}/kubespray/inventory.ini
  register: inventory_contents
  changed_when: false
  when: kubernetes_deployment.enabled | default(false)

- name: Display debug information
  debug:
    msg: |
      Current Directory: {{ current_dir.stdout }}
      Inventory Path: {{ inventory_dir }}/kubespray/inventory.ini
      Kubespray Dir: {{ kubespray_dir }}
      Group Vars Path: {{ inventory_dir }}/kubespray/group_vars/all/all.yml
  when: kubernetes_deployment.enabled | default(false)

- name: Check SSH key file exists
  stat:
    path: "{{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}"
  register: ssh_key_stat
  delegate_to: localhost
  when: kubernetes_deployment.enabled | default(false)

- name: Fail if SSH key not found
  fail:
    msg: "SSH key file not found at {{ kubernetes_deployment.ssh_key_path }}. Please ensure the key file exists and the path is correct."
  when: 
    - kubernetes_deployment.enabled | default(false)
    - not ssh_key_stat.stat.exists

- name: Set SSH key permissions
  file:
    path: "{{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}"
    mode: '0600'
  delegate_to: localhost
  when: 
    - kubernetes_deployment.enabled | default(false)
    - ssh_key_stat.stat.exists

- name: Test SSH connection to all hosts
  command:
    cmd: "ssh -i {{ kubernetes_deployment.ssh_key_path | expanduser | realpath }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ item.ansible_user | default(kubernetes_deployment.default_ansible_user) }}@{{ item.ansible_host }} 'echo SSH connection successful'"
  loop: "{{ kubernetes_deployment.control_plane_nodes + kubernetes_deployment.worker_nodes | default([]) }}"
  register: ssh_test
  changed_when: false
  ignore_errors: true
  delegate_to: localhost
  when: kubernetes_deployment.enabled | default(false)

- name: Display SSH test results
  debug:
    msg: |
      SSH test results:
      {% for result in ssh_test.results %}
      Host: {{ result.item.name }} ({{ result.item.ansible_host }})
      Status: {{ 'Success' if result.rc == 0 else 'Failed' }}
      {% if result.rc != 0 %}
      Error: {{ result.stderr }}
      {% endif %}
      {% endfor %}
  when: 
    - kubernetes_deployment.enabled | default(false)
    - ssh_test is defined

- name: Set Ansible environment variables
  set_fact:
    ansible_env_vars: "{{ kubespray_env | combine(kubespray_extra_env) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Set default values for async execution
  set_fact:
    kubespray_async_timeout: "{{ kubernetes_deployment.async_config.timeout | default(3600) }}"
    kubespray_poll_interval: "{{ kubernetes_deployment.async_config.poll_interval | default(10) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Clear Ansible fact cache to prevent IP conflicts
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - "~/.ansible/tmp"
    - "~/.ansible/cp"
    - "/tmp/ansible-facts-*"
  ignore_errors: true
  delegate_to: localhost
  when: kubernetes_deployment.enabled | default(false)

- name: Force refresh network facts on target hosts
  setup:
    gather_subset: '!all,network'
    filter: "ansible_default_ipv4"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ item.ansible_user | default(kubernetes_deployment.default_ansible_user) }}"
  loop: "{{ kubernetes_deployment.control_plane_nodes + kubernetes_deployment.worker_nodes | default([]) }}"
  ignore_errors: true
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars directory for explicit IP configuration
  file:
    path: "{{ inventory_dir }}/kubespray/host_vars"
    state: directory
    mode: '0755'
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars files for control plane nodes with explicit IP settings
  template:
    src: "{{ playbook_dir }}/templates/host_vars.yml.j2"
    dest: "{{ inventory_dir }}/kubespray/host_vars/{{ item.name }}.yml"
  vars:
    node_private_ip: "{{ item.private_ip }}"
    node_access_ip: "{{ item.private_ip }}"
  loop: "{{ kubernetes_deployment.control_plane_nodes }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Create host_vars files for worker nodes with explicit IP settings
  template:
    src: "{{ playbook_dir }}/templates/host_vars.yml.j2"
    dest: "{{ inventory_dir }}/kubespray/host_vars/{{ item.name }}.yml"
  vars:
    node_private_ip: "{{ item.private_ip }}"
    node_access_ip: "{{ item.private_ip }}"
  loop: "{{ kubernetes_deployment.worker_nodes | default([]) }}"
  when: kubernetes_deployment.enabled | default(false)

- name: Verify IP configuration before deployment
  debug:
    msg: |
      === IP Configuration Verification ===
      Control Plane Nodes:
      {% for node in kubernetes_deployment.control_plane_nodes %}
      - {{ node.name }}: 
        - ansible_host (public): {{ node.ansible_host }}
        - private_ip (etcd only): {{ node.private_ip }}
        - host_vars file: {{ inventory_dir }}/kubespray/host_vars/{{ node.name }}.yml
      {% endfor %}
      {% if kubernetes_deployment.worker_nodes is defined and kubernetes_deployment.worker_nodes|length > 0 %}
      Worker Nodes:
      {% for node in kubernetes_deployment.worker_nodes %}
      - {{ node.name }}:
        - ansible_host (public): {{ node.ansible_host }}
        - private_ip (etcd only): {{ node.private_ip }}
        - host_vars file: {{ inventory_dir }}/kubespray/host_vars/{{ node.name }}.yml
      {% endfor %}
      {% endif %}
      
      IMPORTANT: Only etcd-specific variables are overridden to fix binding issues.
      API Server will remain accessible on public IP: {{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}
  when: kubernetes_deployment.enabled | default(false)

- name: Set node IP variables
  set_fact:
    control_plane_ips: >
      {{ kubernetes_deployment.control_plane_nodes | map(attribute='private_ip') | 
         map('default', ansible_default_ipv4.address) | list }}
    worker_node_ips: >
      {{ kubernetes_deployment.worker_nodes | default([]) | map(attribute='private_ip') | 
         map('default', ansible_default_ipv4.address) | list }}
    first_control_plane_ip: >
      {{ kubernetes_deployment.control_plane_nodes[0].private_ip | 
         default(hostvars[groups['kube_control_plane'][0]].ansible_default_ipv4.address) }}
  when: kubernetes_deployment.enabled | default(false)

- name: Track Kubernetes deployment start
  set_fact:
    installation_summary:
      total_items: "{{ (installation_summary.total_items | int) + 1 }}"
      successful_items: "{{ installation_summary.successful_items + [deployment_info] }}"
      failed_items: "{{ installation_summary.failed_items }}"
      skipped_items: "{{ installation_summary.skipped_items }}"
      kubernetes_components: "{{ installation_summary.kubernetes_components + [deployment_info] }}"
      helm_charts: "{{ installation_summary.helm_charts }}"
      manifests: "{{ installation_summary.manifests }}"
      kubectl_commands: "{{ installation_summary.kubectl_commands }}"
      command_executions: "{{ installation_summary.command_executions }}"
  vars:
    deployment_info:
      name: "kubespray-cluster-deployment"
      type: "kubernetes"
      status: "success"
      timestamp: "{{ ansible_date_time.iso8601 }}"
      details: "Starting Kubespray cluster deployment with {{ kubernetes_deployment.control_plane_nodes | length }} control plane nodes"
  when: 
    - summary_enabled | default(true)
    - kubernetes_deployment.enabled | default(false)

- name: Create async directory
  file:
    path: "/home/{{ kubernetes_deployment.default_ansible_user }}/.ansible_async"
    state: directory
    mode: '0755'
    owner: "{{ kubernetes_deployment.default_ansible_user }}"
    group: "{{ kubernetes_deployment.default_ansible_user }}"
  become: true
  when: kubernetes_deployment.enabled | default(false)

- name: Create Kubespray vars file
  template:
    src: kubespray_vars.yml.j2
    dest: /tmp/kubespray-vars.yml
  delegate_to: localhost

- name: Ensure Ansible async directory exists
  file:
    path: "/tmp/ansible-async"
    state: directory
    mode: '0755'
  delegate_to: localhost

- name: Run Kubespray cluster deployment
  command:
    cmd: >-
      ansible-playbook -i {{ inventory_dir }}/kubespray/inventory.ini cluster.yml
      -e @/tmp/kubespray-vars.yml
      -e @/tmp/kubespray-extra-vars.json
      -e ansible_ssh_private_key_file={{ kubernetes_deployment.ssh_key_path | expanduser | realpath }}
      -e ansible_user={{ kubernetes_deployment.default_ansible_user | default('root') }}
      -e ansible_become_pass="{{ kubernetes_deployment.ansible_sudo_pass | default('') }}"
      -e ansible_async_dir=/tmp/ansible-async
      --become
      --become-method=sudo
      --become-user=root
      --ssh-extra-args="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
      -vvvv
  args:
    chdir: "{{ kubespray_dir }}"
  async: "{{ kubernetes_deployment.async_config.timeout | default(3600) }}"
  poll: "{{ kubernetes_deployment.async_config.poll_interval | default(5) }}"
  register: kubespray_deployment
  delegate_to: localhost
  environment:
    ANSIBLE_LOG_PATH: "{{ playbook_dir }}/ansible.log"
    ANSIBLE_CALLBACK_PLUGINS: "{{ playbook_dir }}/callback_plugins"
    ANSIBLE_STDOUT_CALLBACK: debug
    ANSIBLE_DISPLAY_SKIPPED_HOSTS: "True"
    ANSIBLE_DISPLAY_OK_HOSTS: "True"
    ANSIBLE_SHOW_CUSTOM_STATS: "True"
    ANSIBLE_LOAD_CALLBACK_PLUGINS: "True"

- name: Wait for Kubespray deployment
  async_status:
    jid: "{{ kubespray_deployment.ansible_job_id }}"
  register: job_result
  until: job_result.finished
  retries: "{{ (kubernetes_deployment.async_config.timeout | default(3600)) | int // (kubernetes_deployment.async_config.poll_interval | default(5)) | int }}"
  delay: "{{ kubernetes_deployment.async_config.poll_interval | default(5) }}"
  when: kubespray_deployment.ansible_job_id is defined
  delegate_to: localhost

- name: Check deployment result
  fail:
    msg: "Kubespray deployment failed. Check ansible.log for details."
  when: job_result.rc is defined and job_result.rc != 0

- name: Verify cluster access
  command:
    cmd: "kubectl cluster-info"
  register: cluster_info
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  remote_user: "{{ kubernetes_deployment.default_ansible_user }}"
  ignore_errors: true
  environment:
    KUBECONFIG: "/etc/kubernetes/admin.conf"

- name: Display cluster information
  debug:
    var: cluster_info.stdout_lines
  when: cluster_info.rc == 0

- name: Get cluster node status
  command:
    cmd: "kubectl get nodes -o wide"
  register: node_status
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  remote_user: "{{ kubernetes_deployment.default_ansible_user }}"
  ignore_errors: true
  environment:
    KUBECONFIG: "/etc/kubernetes/admin.conf"

- name: Display node status
  debug:
    var: node_status.stdout_lines
  when: node_status.rc == 0

- name: Get cluster pod status
  command:
    cmd: "kubectl get pods -A -o wide"
  register: pod_status
  delegate_to: "{{ kubernetes_deployment.control_plane_nodes[0].ansible_host }}"
  remote_user: "{{ kubernetes_deployment.default_ansible_user }}"
  ignore_errors: true
  environment:
    KUBECONFIG: "/etc/kubernetes/admin.conf"

- name: Display pod status
  debug:
    var: pod_status.stdout_lines
  when: pod_status.rc == 0

- name: Get current user
  command: whoami
  register: current_user
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get current user's group
  command: id -gn
  register: current_group
  changed_when: false
  delegate_to: localhost
  become: false

- name: Get real home directory
  command: "echo $HOME"
  register: real_home
  changed_when: false
  become: false
  delegate_to: localhost

- name: Ensure output directory exists
  file:
    path: "{{ lookup('env', 'PWD') }}/output"
    state: directory
    mode: '0755'
  become: false
  when: kubernetes_deployment.enabled | default(false)

- name: Fix output directory ownership
  block:
    - name: Try fixing directory ownership without sudo
      file:
        path: "{{ lookup('env', 'PWD') }}/output"
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
        recurse: yes
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: dir_ownership_result

    - name: Fix directory ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ lookup('env', 'PWD') }}/output
        sudo chmod -R 755 {{ lookup('env', 'PWD') }}/output
      delegate_to: localhost
      ignore_errors: true
      when: dir_ownership_result is failed

- name: Create temporary directory on remote host
  raw: mkdir -p /tmp/kubeconfig-temp && chmod 755 /tmp/kubeconfig-temp
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true

- name: Wait for kubeconfig file to be created
  shell: |
    timeout 600 bash -c 'while [ ! -f /etc/kubernetes/admin.conf ]; do sleep 5; echo "Waiting for kubeconfig..."; done'
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  register: kubeconfig_wait
  ignore_errors: true

- name: Copy kubeconfig to temporary location with sudo
  shell: |
    cp /etc/kubernetes/admin.conf /tmp/kubeconfig-temp/admin.conf && \
    chmod 644 /tmp/kubeconfig-temp/admin.conf && \
    chown {{ kubernetes_deployment.default_ansible_user }}:{{ kubernetes_deployment.default_ansible_user }} /tmp/kubeconfig-temp/admin.conf
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  ignore_errors: true
  register: kubeconfig_copy_result
  when: kubeconfig_wait is success

- name: Display warning if kubeconfig copy failed
  debug:
    msg: "WARNING: Failed to copy kubeconfig from /etc/kubernetes/admin.conf. This is expected if the cluster is not yet fully initialized."
  when: kubeconfig_copy_result is failed or kubeconfig_wait is failed

- name: Fetch kubeconfig from temporary location
  fetch:
    src: "/tmp/kubeconfig-temp/admin.conf"
    dest: "{{ lookup('env', 'PWD') }}/output/kubeconfig"
    flat: yes
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
  register: kubeconfig_fetch
  when: kubeconfig_copy_result is success

- name: Clean up temporary directory
  file:
    path: "/tmp/kubeconfig-temp"
    state: absent
  delegate_to: "{{ groups['kube_control_plane'][0] }}"
  vars:
    ansible_ssh_private_key_file: "{{ kubernetes_deployment.ssh_key_path }}"
    ansible_user: "{{ kubernetes_deployment.default_ansible_user }}"
    ansible_become: true
  when: kubeconfig_fetch is success

- name: Fix all local file permissions and ownership
  block:
    - name: Try fixing permissions and ownership without sudo
      file:
        path: "{{ item }}"
        mode: '0644'  # Changed to allow read access for others
        owner: "{{ lookup('env', 'USER') }}"
        group: "{{ lookup('env', 'USER') }}"
      with_items:
        - "{{ lookup('env', 'PWD') }}/output/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      become: false
      ignore_errors: true
      register: fix_perms_result

    - name: Fix permissions and ownership with sudo if needed
      shell: |
        sudo chown -R {{ lookup('env', 'USER') }}:{{ lookup('env', 'USER') }} {{ item }}
        sudo chmod 644 {{ item }}  # Explicit permission setting
      with_items:
        - "{{ lookup('env', 'PWD') }}/output/kubeconfig"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
      when: fix_perms_result is failed

    - name: Ensure directories have execute permission
      shell: |
        sudo chmod 755 {{ item }}
      with_items:
        - "{{ lookup('env', 'PWD') }}/output"
        - "{{ lookup('env', 'PWD') }}/inventory"
        - "{{ lookup('env', 'PWD') }}/inventory/kubespray"
      delegate_to: localhost
      ignore_errors: true
  when: kubeconfig_fetch is success

- name: Verify kubeconfig exists
  stat:
    path: "{{ lookup('env', 'PWD') }}/output/kubeconfig"
  register: kubeconfig_stat
  delegate_to: localhost

- name: Display kubeconfig location
  debug:
    msg: |
      Kubeconfig has been copied to:
      - Global location: {{ lookup('env', 'PWD') }}/output/kubeconfig
      
      You can now use kubectl with:
      export KUBECONFIG={{ lookup('env', 'PWD') }}/output/kubeconfig
      kubectl get nodes
  when: kubeconfig_stat.stat.exists 