# user_input.yml
---
# Kubernetes Deployment Configuration
# Controls the setup and configuration of the Kubernetes cluster using Kubespray
kubernetes_deployment:
  enabled: true                               # Enable/disable Kubernetes cluster deployment
  
  # Deployment Timeout Configuration
  deployment_timeout: 3600                    # Maximum time in seconds for Kubespray deployment (default: 1 hour)
                                              # Large clusters may need 7200 (2 hours) or more
                                              # Small clusters typically complete in 1800-3600 seconds (30-60 minutes)
  poll_interval: 30                          # Polling interval in seconds to check deployment status (default: 30 seconds)
                                              # Lower values (10-15) provide more frequent updates but increase log verbosity
                                              # Higher values (60-120) reduce log spam but provide less frequent updates
  
  # Firewall Configuration
  firewall:
    enabled: true                             # Enable/disable firewall configuration
    allow_additional_ports: []                # Additional ports to allow (e.g., ["8080", "9090"])
    
  # NVIDIA Runtime Configuration
  nvidia_runtime:
    enabled: true                            # Enable/disable NVIDIA runtime configuration
    install_toolkit: true                     # Install NVIDIA Container Toolkit if not present
    configure_containerd: true                # Configure containerd with NVIDIA runtime
    create_runtime_class: true                # Create Kubernetes RuntimeClass for NVIDIA
  
  # SSH Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/root/.ssh/k8s_rsa"     # Absolute Path to SSH private key for node access
  default_ansible_user: "avesha"                 # Default SSH user for node access
  
  # Node Configuration
  # Define the control plane (master) nodes for the Kubernetes cluster
  control_plane_nodes:                         # List of control plane nodes
    - name: master-k8s                        # Hostname/identifier for the node
      ansible_host: 89.169.114.118
              # IP address or DNS name of the node
      ansible_user: avesha
      ansible_become: true
      ansible_become_method: sudo
      ansible_become_user: root
      private_ip: 10.0.102.57        # SSH user for this specific node
  
  # Kubernetes Components Configuration
  network_plugin: calico                       # CNI plugin for pod networking (options: calico, flannel, etc.)
  container_runtime: containerd                # Container runtime (options: containerd, docker)
  dns_mode: coredns                           # DNS service for the cluster

# Global image pull secret settings
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"
  username: ""
  password: ""

# Kubeconfig settings
global_kubeconfig: "files/kubeconfig"
global_kubecontext: "kubecontext" 
use_global_context: true

# Helm repository settings
use_local_charts: false
local_charts_path: "charts"
global_chart_repo_url: ""
global_repo_username: ""
global_repo_password: ""
readd_helm_repos: true

# NGC and Avesha credentials from environment variables
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') }}"
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') }}"
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') }}"
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') }}"

# Validation Configuration
validate_prerequisites:
  enabled: true            

# Execution Configuration
execution_order_enabled: true               # Enable/disable execution order tasks

# Execution configuration
execution_order:
  - gpu_operator_chart
  - prometheus_stack
  - pushgateway_manifest
  - keda_chart
  - nim_operator_chart
  - create_ngc_secrets
  - verify_ngc_secrets
  - create_avesha_secret
  - nim_cache_manifest
  - nim_service_manifest
  - keda_scaled_object_manifest
  - create_inference_pod_configmap
  - smart_scaler_inference
  - create_locust_configmap
  - locust_manifest

helm_charts:
  gpu_operator_chart:
    release_name: gpu-operator
    chart_ref: gpu-operator
    release_namespace: gpu-operator
    create_namespace: true
    wait: true
    chart_version: v25.3.0
    release_values:
      mig:
        strategy: none
      dcgm:
        enabled: true
      driver:
        enabled: false
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"

  prometheus_stack:
    release_name: prometheus
    chart_ref: kube-prometheus-stack
    release_namespace: monitoring
    create_namespace: true
    wait: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false

      prometheus:
        prometheusSpec:
          retention: 15d
          additionalScrapeConfigs:
            - job_name: gpu-metrics
              scrape_interval: 1s
              metrics_path: /metrics
              scheme: http
              kubernetes_sd_configs:
                - role: endpoints
                  namespaces:
                    names:
                      - monitoring
                      - gpu-operator
              relabel_configs:
                - source_labels: [__meta_kubernetes_endpoints_name]
                  action: drop
                  regex: .*-node-feature-discovery-master
                - source_labels: [__meta_kubernetes_pod_node_name]
                  action: replace
                  target_label: kubernetes_node

      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true

      kubelet:
        serviceMonitor:
          https: false

      grafana:
        enabled: true
        persistence:
          enabled: true
          size: 1Gi

      defaultRules:
        rules:
          etcd: false

  keda_chart:
    release_name: keda
    chart_ref: keda
    release_namespace: keda
    create_namespace: true
    wait: true
    chart_repo_url: "https://kedacore.github.io/charts"
    chart_version: "2.12.1"

  nim_operator_chart:
    release_name: nim
    chart_ref: k8s-nim-operator
    release_namespace: nim
    create_namespace: true
    wait: true
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    chart_version: "v1.0.1"

manifests:
  pushgateway_manifest:
    name: pushgateway-setup
    manifest_file: "files/pushgateway.yaml"
    namespace: pushgateway-system
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    variables:
      namespace: monitoring
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true

  nim_cache_manifest:
    name: nim-cache-setup
    manifest_file: "files/nim-cache.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_cache_name: "meta-llama3-8b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.4"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "vllm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest:
    name: nim-service-setup
    manifest_file: "files/nim-service.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_service_name: "meta-llama3-8b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
        - name: "MAX_NUM_SEQS"
          value: "128"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"
      nim_service_image_tag: "1.8.4"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-8b-instruct"
      nim_service_storage_cache_profile: "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest:
    name: keda-scaled-object-setup
    manifest_file: "files/keda-scaled-object.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      keda_scaled_object_name: "llm-demo-keda"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-8b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: >-
        smartscaler_hpa_num_pods{job="pushgateway", kubernetes_pod_name="meta-llama3-8b-instruct->nim->nim-llama", ss_deployment_name="meta-llama3-8b-instruct"}

  smart_scaler_inference:
    name: smart-scaler-inference-setup
    manifest_file: "files/smart-scaler-inference.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      smart_scaler_name: "smart-scaler-llm-inf"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app"
        cluster_name: "nim-llama"
        tenant_id: "tenant-b200-local"
        app_name: "nim-llama"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config-inference.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest:
    name: locust-setup
    manifest_file: "files/locust-deploy.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      locust_name: "locust-load"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-8b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile"

# Command execution configuration
command_exec:
  - name: "create_ngc_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      # First check if secrets exist
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim get secret ngc-secret >/dev/null 2>&1; then
            echo "Secret ngc-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim delete secret ngc-secret
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim create secret docker-registry ngc-secret \
            --docker-server=nvcr.io \
            --docker-username='$oauthtoken' \
            --docker-password="${NGC_DOCKER_API_KEY}" \
            --docker-email='your.email@solo.io'
        env:
          NGC_DOCKER_API_KEY: "{{ ngc_docker_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
      
      # Handle ngc-api-secret
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret ngc-api-secret -n nim >/dev/null 2>&1; then
            echo "Secret ngc-api-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              delete secret ngc-api-secret -n nim
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create secret generic ngc-api-secret -n nim\
            --from-literal=NGC_API_KEY="${NGC_API_KEY}"
        env:
          NGC_API_KEY: "{{ ngc_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}" 

  - name: "verify_ngc_secrets"
    commands:
      - cmd: "kubectl get secret ngc-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true
      - cmd: "kubectl get secret ngc-api-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true 

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi
          
          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"


  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_inference_pod_configmap"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config >/dev/null 2>&1; then
            echo "ConfigMap mesh-config exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config --from-file=files/config-inference.json

  - name: "create_locust_configmap"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile >/dev/null 2>&1; then
            echo "ConfigMap locustfile exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile --from-file=locustfile.py=files/locust.py

