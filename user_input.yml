---
################################################################################
# USER INPUT CONFIGURATION
# Last updated: OCT 6, 2025
################################################################################

###############################################################################
# KUBERNETES DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new Kubernetes cluster using Kubespray
###############################################################################
kubernetes_deployment:
  # Master switch for Kubernetes deployment
  enabled: false                           # Enable kubernetes deployment
  
  # API Server Configuration
  # Defines how clients will connect to the Kubernetes API server
  api_server:
    host: "PUBLIC_IP"        # Load balancer VIP or first master for initial setup
    port: 6443                   # Standard Kubernetes API port (will be load balanced)
                                # Standard Kubernetes API port - change only if required
    bind_address: "0.0.0.0"      # Listen on all interfaces
    secure: true                 # Whether to use HTTPS for API server connections
                                # Should always be true in production environments

  # SSH Access Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/absolute/path/to/.ssh/k8s_rsa"  # Full path to the SSH private key file
                                            # Used to authenticate with the cluster nodes
  default_ansible_user: "REPLACE_SSH_USER"         # Default SSH user for all nodes
                                           # Common values: ubuntu (Ubuntu), ec2-user (AWS)
  # Ansible Sudo Password Configuration
  # Optional: Can be set here, via ANSIBLE_SUDO_PASS environment variable,
  # or will prompt if neither is set
  ansible_sudo_pass: ""  # Leave empty to use environment variable or prompt


  # Control Plane Node Configuration (Multi-Master HA Support)
  # Minimum 3 nodes for HA, always use odd numbers (3, 5, 7)
  control_plane_nodes:
    - name: "master-1"                # Unique identifier for the control plane node
      ansible_host: "PUBLIC_IP"  # Replace with actual master-1 public IP
      ansible_user: "root"      # SSH username for this specific node
                                     # Overrides default_ansible_user if different
      ansible_become: true            # Enable privilege escalation (sudo)
      ansible_become_method: "sudo"     # Method for privilege escalation
      ansible_become_user: "root"       # Target user for privilege escalation
      private_ip: "PRIVATE_IP"      # Replace with actual master-1 private IP
                                     # Used for internal cluster communication
    # # Add more masters for HA (uncomment and configure):
    # - name: "master-2"
    #   ansible_host: "PUBLIC_IP_2"
    #   ansible_user: "root"
    #   ansible_become: true
    #   ansible_become_method: "sudo"
    #   ansible_become_user: "root"
    #   private_ip: "PRIVATE_IP_2"
    # - name: "master-3"
    #   ansible_host: "PUBLIC_IP_3"
    #   ansible_user: "root"
    #   ansible_become: true
    #   ansible_become_method: "sudo"
    #   ansible_become_user: "root"
    #   private_ip: "PRIVATE_IP_3"

  # Worker Node Configuration (Optional but recommended)
  # worker_nodes: []
  # Example worker node configuration:
  # worker_nodes:
  #   - name: "worker-1"
  #     ansible_host: "WORKER_PUBLIC_IP"
  #     ansible_user: "root"
  #     ansible_become: true
  #     ansible_become_method: "sudo"
  #     ansible_become_user: "root"
  #     private_ip: "WORKER_PRIVATE_IP"


    # Python Configuration
  python_config:
    interpreter_path: "/usr/bin/python3"  # Path to Python interpreter
                                     # Used for internal cluster communication
  # Kubernetes Network Configuration
  # Defines the network ranges for various Kubernetes components
  network_config:
    service_subnet: "10.233.0.0/18"  # CIDR range for Kubernetes services
                                    # Must not overlap with pod_subnet or node network
    pod_subnet: "10.233.64.0/18"    # CIDR range for Kubernetes pods
                                   # Must not overlap with service_subnet or node network
    node_prefix: 24                 # Subnet mask for node network
                                  # /24 allows 256 IP addresses per subnet

  # Firewall Configuration
  # Controls access to cluster nodes
  firewall:
    enabled: false                            # Whether to configure firewall rules
    allow_additional_ports:
      - "80"      # HTTP
      - "443"     # HTTPS
      - "6443"    # Kubernetes API server
      - "2379"    # etcd client API
      - "2380"    # etcd server API
      - "10250"   # Kubelet API
      - "10251"   # kube-scheduler
      - "10252"   # kube-controller-manager

  # NVIDIA Container Runtime Configuration
  # Required for GPU support in the cluster
  nvidia_runtime:
    enabled: true                           # Enable NVIDIA container runtime
                                          # Set to true if using NVIDIA GPUs
    install_toolkit: true                   # Install NVIDIA Container Toolkit
                                          # Required for GPU support
    configure_containerd: true              # Configure containerd for NVIDIA runtime
    create_runtime_class: true              # Create Kubernetes RuntimeClass for NVIDIA
    architecture: "amd64"                   # Architecture for NVIDIA container runtime package

  # Kubernetes Component Configuration
  # Core Kubernetes infrastructure choices
  network_plugin: "calico"                  # Container Network Interface (CNI) choice
                                          # Options: calico, flannel, weave, cilium
  
  container_runtime: "containerd"           # Container runtime for Kubernetes
                                          # containerd is the current standard
  
  dns_mode: "coredns"                      # DNS service for the cluster
                                         # CoreDNS is the default since Kubernetes 1.13
                                         
  # Kubelet Configuration
  # Controls node-level settings for the Kubernetes node agent
  kubelet_config:
    max_pods: 256                          # Maximum number of pods per node
                                         # Default is 110, can be increased for high-density nodes
                                         # Consider network and resource constraints when increasing
  # Async Execution Configuration
  # Controls timeout and polling for long-running tasks
  async_config:
    timeout: 3600                          # Maximum time (in seconds) to wait for task completion
    poll_interval: 5                       # How often (in seconds) to check task status

  # Load Balancer Configuration
  # ============================================================================
  # DEFAULT STRATEGY: Load Balancer ALWAYS ENABLED
  # ============================================================================
  # WHY: Future-proof scaling, health monitoring, operational consistency
  # WORKS: Single master (1 node) and multi-master (3+ nodes) deployments
  # FLOW: kubectl → haproxy:6443 → API server:6444 (on each master)
  # ============================================================================
  load_balancer:
    enabled: true                          # DEFAULT: Always enabled (recommended best practice)
    type: "localhost"                      # DEFAULT: Use localhost haproxy proxy
    
    # Localhost Load Balancer Configuration (DEFAULT: Enabled for all deployments)
    # Benefits: Future-proof scaling, health monitoring, consistent architecture
    localhost:
      enabled: true                        # DEFAULT: Always enabled (works for single/multi-master)
      lb_type: "haproxy"                   # Using HAProxy for enterprise-grade load balancing
      bind_address: "0.0.0.0"             # DEFAULT: Bind to all interfaces
      api_port_offset: 1                  # DEFAULT: API server port = client_port + offset (6443+1=6444)
      pod_name: "haproxy"                 # HAProxy load balancer pod name
      healthcheck_port: 8081              # DEFAULT: Health monitoring port
      keepalive_timeout: "5m"             # DEFAULT: Connection timeout
      memory_requests: "32M"              # DEFAULT: Memory allocation for LB pod
      cpu_requests: "25m"                 # DEFAULT: CPU allocation for LB pod
    
    # # External Load Balancer Configuration (ALTERNATIVE: For external LB like HAProxy/F5)
    # external:
    #   enabled: false                       # ALTERNATIVE: Use only if you have external load balancer
    #   address: ""                         # External LB IP address (defaults to api_server.host)
    #   domain_name: "k8s-api.yourdomain.com"  # Optional: Domain name for LB
    #                                       # External LB port uses api_server.port automatically
    
    # # kube-vip Configuration (ALTERNATIVE: For VIP-based HA)
    # kube_vip:
    #   enabled: false                       # ALTERNATIVE: Use only for VIP-based high availability
    #   controlplane_enabled: true          # Enable kube-vip for control plane
    #   vip_address: ""                     # Virtual IP address (defaults to api_server.host)
    #   interface: "eth0"                   # Network interface for VIP
    #   arp_enabled: true                   # Enable ARP for VIP management
    #   leader_election: true               # Enable leader election
    #   cidr: 24                           # CIDR for VIP network

  # etcd High Availability Configuration
  # Critical for multi-master cluster stability and data consistency
  # NOTE: Disabled for single master setup
  etcd_ha:
    enabled: true                         # Disabled for single master setup
    deployment_type: "kubeadm"            # Deployment type: "kubeadm" or "external"
    
    # etcd Cluster Configuration
    cluster:
      initial_cluster_state: "new"        # Initial cluster state: "new" or "existing"
      heartbeat_interval: "100"           # Heartbeat interval in milliseconds
      election_timeout: "1000"            # Election timeout in milliseconds
      quota_backend_bytes: "2147483648"   # Storage quota (2GB default)
      auto_compaction_retention: "8"      # Auto compaction retention in hours
      max_request_bytes: "1572864"        # Max request size (1.5MB)
      metrics: "basic"                    # Metrics level: "basic" or "extensive"
    
    # etcd Events Cluster (Optional)
    events_cluster:
      enabled: false                      # Disabled for single master
      setup: false                       # Disabled for single master
    
    # etcd Security Configuration
    security:
      peer_auto_tls: false               # Disable peer auto TLS (use proper certs)
      client_cert_auth: true             # Require client certificate authentication
      peer_cert_auth: true               # Require peer certificate authentication
    
    # etcd Performance Tuning
    performance:
      max_snapshots: 5                   # Maximum number of snapshots to retain
      max_wals: 5                        # Maximum number of WAL files to retain
      snapshot_count: 100000             # Snapshot after this many transactions
      wal_dir: ""                        # Custom WAL directory (empty = default)
      data_dir: ""                       # Custom data directory (empty = default)

###############################################################################
# REQUIRED SETTINGS
# These settings must be configured regardless of deployment type
###############################################################################

skip_prerequisites: false

# Validation and Execution Settings
validate_prerequisites:
  enabled: true                              # Required: Must be enabled for validation

execution_order_enabled: true                # Required: Must be enabled for ordered execution

# Required Kubeconfig Settings
global_control_plane_ip: "PUBLIC_IP"         # Provide the public IP for metallb/Nginx
global_kubeconfig: "output/kubeconfig"        # Required: Path to kubeconfig file
global_kubecontext: "kubernetes-admin@cluster.local"  # Required: Kubernetes context
use_global_context: true                     # Required: Use global context

# Required Helm Settings
use_local_charts: true                      # Required: Use local Helm charts
local_charts_path: "files/charts"                  # Required: Path to local charts
global_chart_repo_url: ""                    # Required: Global Helm repository URL
global_repo_username: ""                     # Required: Repo username if using private repos
global_repo_password: ""                     # Required: Repo password if using private repos
readd_helm_repos: true                       # Required: Re-add Helm repos

# Required Credentials
# These will use environment variables if available, otherwise fall back to 'not-set'
# You can override these values directly in this file or use environment variables
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') | default('not-set') }}"              # Required for NGC access
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') | default('not-set') }}" # Required for NGC Docker registry
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') | default('not-set') }}" # Required for Avesha images
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') | default('not-set') }}" # Required for Avesha images

# Required Execution Order
execution_order:
  # Core Infrastructure
  # - metallb_chart                 
  # - metallb_l2_config            
  # - metallb_ip_pool              
  # - nginx_ingress_config         
  # - nginx_ingress_chart
  # - cert_manager
  # - local_path_retain_storageclass  

  # Base Applications
  - gpu_operator_chart
  - prometheus_stack
  - pushgateway_manifest
  - create_avesha_secret

  # AMD GPU Support
  # - amd_gpu_operator_chart         # Install AMD GPU operator for AMD GPU support,enable cert-manager as well from above section.
  # - amd_gpu_deviceconfig_manifest  # Configure AMD GPU device settings
  
  # EGS Application
  # - kubeslice_controller_egs  # Install EGS controller first
  # - kubeslice_ui_egs         # Install EGS UI after controller
  # - egs_project_manifest              # Create EGS project
  # - egs_cluster_registration_worker_1 # Register worker-1 cluster
  # - fetch_worker_secret_worker_1 # Fetch worker secret after registration
  # - kubeslice_worker_egs_worker_1 # Install worker after fetching secret


###############################################################################
# OPTIONAL CONFIGURATION
###############################################################################

# MetalLB Configuration
metallb:
  enabled: false                      # Enable MetalLB
  namespace: "metallb"                   # Namespace for MetalLB deployment
  address_range: 172.18.1.1-172.18.1.16  # Reference global IP
  protocol: "layer2"                     # Use Layer 2 mode

# Nginx Ingress Configuration
nginx_ingress:
  enabled: false
  namespace: "ingress-nginx"
  replica_count: 1
  service_type: "LoadBalancer"
  metrics_enabled: true
  default_ssl_certificate: false
  enable_proxy_protocol: false
  enable_real_ip: true

# Optional Docker Registry Configuration
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"  # Optional: Docker registry URL
  username: "{{ avesha_docker_username }}"
  password: "{{ avesha_docker_password }}"

###############################################################################
# HELM CHARTS CONFIGURATION
###############################################################################

helm_charts:

  # Cert Manager Chart Configuration
  cert_manager:
    release_name: "cert-manager"
    chart_ref: "./cert-manager"
    release_namespace: "cert-manager"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "v1.13.3"
    release_values:
      installCRDs: true
      global:
        logLevel: 2
      prometheus:
        enabled: false
        servicemonitor:
          enabled: false
      webhook:
        timeoutSeconds: 30

  # EGS Controller Chart Configuration
  kubeslice_controller_egs:
    release_name: "egs-controller"
    chart_ref: "./kubeslice-controller-egs"    # Using relative path
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
        namespaceConfig:
          labels: {}
          annotations: {}
        kubeTally:
          enabled: false
          postgresSecretName: kubetally-db-credentials
          postgresAddr: "kt-postgresql.kt-postgresql.svc.cluster.local"
          postgresPort: 5432
          postgresUser: "postgres"
          postgresPassword: "postgres"
          postgresDB: "postgres"
          postgresSslmode: disable
          prometheusUrl: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      kubeslice:
        controller:
          endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # EGS UI Chart Configuration
  kubeslice_ui_egs:
    release_name: "egs-ui"
    chart_ref: "./kubeslice-ui-egs"           # Using relative path
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
      kubeslice:
        prometheus:
          url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
        uiproxy:
          service:
            type: NodePort
          labels:
            app: kubeslice-ui-proxy
          annotations: {}
          ingress:
            enabled: false
            servicePort: 443
            className: ""
            hosts:
              - host: ui.kubeslice.com
                paths:
                  - path: /
                    pathType: Prefix
            tls: []
            annotations: []
            extraLabels: {}
        egsCoreApis:
          enabled: true
          service:
            type: ClusterIP
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # Kubeslice Worker Chart Configuration
  kubeslice_worker_egs_worker_1:
    release_name: "egs-worker"
    chart_ref: "./kubeslice-worker-egs"       # Using relative path
    release_namespace: "kubeslice-system"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    values_files:
      - "output/worker-1-values.yaml"
    release_values:
      cluster:
        name: "worker-1"
        endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      global:
        imageRegistry: "docker.io/aveshasystems"
      egs:
        prometheusEndpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        grafanaDashboardBaseUrl: "http://<grafana-lb>/d/Oxed_c6Wz"
      metrics:
        insecure: true
      kserve:
        enabled: false
        kserve:
          controller:
            gateway:
              domain: "kubeslice.com"
              ingressGateway:
                className: "nginx"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # MetalLB Chart Configuration
  metallb_chart:
    release_name: "metallb"
    chart_ref: "metallb"
    release_namespace: "{{ metallb.namespace }}"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://metallb.github.io/metallb"
    chart_version: "0.13.10"
    release_values:
      prometheus:
        serviceMonitor:
          enabled: false
      controller:
        logLevel: info
      speaker:
        logLevel: info
        frr:
          enabled: false
      webhookConfiguration:
        enabled: true
        timeoutSeconds: 15
      crds:
        enabled: true
      rbac:
        create: true

  # Nginx Ingress Controller Chart
  nginx_ingress_chart:
    release_name: "ingress-nginx"
    chart_ref: "ingress-nginx"
    release_namespace: "ingress-nginx"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://kubernetes.github.io/ingress-nginx"
    chart_repo_name: "ingress-nginx"
    chart_version: "4.7.1"
    release_values:
      controller:
        replicaCount: 1
        service:
          enabled: true
          type: "NodePort"
        metrics:
          enabled: true
          serviceMonitor:
            enabled: false
        config:
          use-proxy-protocol: false
          real-ip-header: "proxy_protocol"
          proxy-real-ip-cidr: "0.0.0.0/0"
          use-forwarded-headers: "true"
        publishService:
          enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
            timeoutSeconds: 15
        resources:
          requests:
            cpu: "1000m"
            memory: "1024Mi"
          limits:
            cpu: "2000m"
            memory: "2048Mi"
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
      defaultBackend:
        enabled: false
        service:
          type: NodePort
        resources:
          requests:
            cpu: "1000m"
            memory: "2048"
          limits:
            cpu: "2000m"
            memory: "4096Mi"

  gpu_operator_chart:
    release_name: "gpu-operator"
    chart_ref: "gpu-operator"
    release_namespace: "gpu-operator"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_version: "v25.3.0"
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    dependencies: []
    release_values:
      mig:
        strategy: "none"
      dcgm:
        enabled: true
      driver:
        enabled: false
      cdi:
        default: true
        enabled: true
      hostPaths:
        driverInstallDir: "/home/kubernetes/bin/nvidia"
      toolkit:
        installDir: "/home/kubernetes/bin/nvidia"
        env:
          - name: "CONTAINERD_RUNTIME_CLASS"
            value: "nvidia"

  # AMD GPU Operator Chart
  amd_gpu_operator_chart:
    release_name: "amd-gpu-operator"
    chart_ref: "./amd-gpu-operator"
    release_namespace: "amd-gpu-operator"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []

  # Prometheus Stack
  prometheus_stack:
    release_name: "prometheus"
    chart_ref: "kube-prometheus-stack"
    release_namespace: "monitoring"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: false
    atomic: false
    skip_diff: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false
      prometheus:
        service:
          type: NodePort
        prometheusSpec:
          retention: "15d"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: local-path
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 10Gi
          additionalScrapeConfigs:
            - job_name: "gpu-metrics"
              scrape_interval: "1s"
              metrics_path: "/metrics"
              scheme: "http"
              kubernetes_sd_configs:
                - role: "endpoints"
                  namespaces:
                    names:
                      - "monitoring"
                      - "gpu-operator"
              relabel_configs:
                - source_labels: ["__meta_kubernetes_endpoints_name"]
                  action: "drop"
                  regex: ".*-node-feature-discovery-master"
                - source_labels: ["__meta_kubernetes_pod_node_name"]
                  action: "replace"
                  target_label: "kubernetes_node"
      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
      kubelet:
        serviceMonitor:
          https: false
      grafana:
        enabled: true
        persistence:
          enabled: true
          storageClassName: local-path
          size: 1Gi
          accessModes: ["ReadWriteOnce"]
        service:
          type: NodePort


###############################################################################
# KUBERNETES MANIFESTS CONFIGURATION
###############################################################################

manifests:
  # Local Path Retain StorageClass
  local_path_retain_storageclass:
    name: "local-path-retain-storageclass"
    manifest_file: "files/local-path-retain-storageclass.yaml.j2"
    namespace: ""  # StorageClass is cluster-scoped
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: []
    variables:
      storage_class_name: "local-path-retain"
      provisioner: "rancher.io/local-path"
      reclaim_policy: "Retain"
      volume_binding_mode: "WaitForFirstConsumer"
      allow_volume_expansion: true
      dir_mode: "0775"
      file_mode: "0664"
      mount_options:
        - "rw"
        - "exec"



  # EGS Project Configuration
  egs_project_manifest:
    name: "egs-project"
    manifest_file: "files/egs-project.yaml.j2"
    namespace: "kubeslice-controller"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - kubeslice_controller_egs
    variables:
      project_name: "avesha"
      project_namespace: "kubeslice-controller"
      admin_user: "admin"

  # EGS Cluster Registration Configuration
  egs_cluster_registration_worker_1:
    name: "egs-cluster-registration-worker-1"
    manifest_file: "files/egs-cluster-registration.yaml.j2"
    namespace: "kubeslice-avesha"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - egs_project_manifest
    variables:
      cluster_name: "worker-1"
      project_name: "avesha"
      telemetry:
        enabled: true
        endpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        telemetryProvider: "prometheus"
      geoLocation:
        cloudProvider: "DATACENTER"
        cloudRegion: ""


  # MetalLB IP Pool Configuration
  metallb_ip_pool:
    name: "metallb-ip-pool"
    manifest_file: "files/metallb-ip-pool.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      metallb_pool_name: "first-pool"
      metallb_namespace: "metallb"
      metallb_address_range: "172.18.1.1-172.18.1.16"  # Using the control plane IP

  # MetalLB L2 Advertisement Configuration
  metallb_l2_config:
    name: "metallb-l2-config"
    manifest_file: "files/metallb-l2-config.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: 
      - metallb_chart  # Ensure MetalLB chart is fully installed first
    variables:
      metallb_l2_name: "l2"
      metallb_namespace: "metallb"
      metallb_pool_name: "first-pool"

  # Nginx Ingress Additional Configuration
  nginx_ingress_config:
    name: "nginx-ingress-config"
    manifest_file: "files/nginx-ingress-config.yaml.j2"
    namespace: "ingress-nginx"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: []
    variables:
      nginx_config_name: "nginx-ingress-controller-custom-config"
      nginx_namespace: "ingress-nginx"
      nginx_proxy_buffer_size: "128k"
      nginx_proxy_body_size: "50m"
      nginx_client_header_buffer_size: "64k"
      nginx_large_client_header_buffers: "4 128k"
      nginx_client_body_buffer_size: "128k"
      nginx_enable_brotli: "true"
      nginx_use_gzip: "true"
      nginx_gzip_level: "6"
      nginx_gzip_types: "application/atom+xml application/javascript application/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/javascript text/plain text/x-component"


  # AMD GPU DeviceConfig Manifest
  amd_gpu_deviceconfig_manifest:
    name: "amd-gpu-deviceconfig"
    manifest_file: "files/amd-gpu-deviceconfig.yaml.j2"
    namespace: "amd-gpu-operator"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - amd_gpu_operator_chart
    variables:
      deviceconfig_name: "amd-deviceconfig"
      deviceconfig_namespace: "amd-gpu-operator"
      driver_enable: false
      device_plugin_image: "rocm/k8s-device-plugin:latest"
      node_labeller_image: "rocm/k8s-device-plugin:labeller-latest"
      metrics_exporter_enable: true
      metrics_exporter_service_type: "NodePort"
      metrics_exporter_node_port: 32500
      metrics_exporter_image: "docker.io/rocm/device-metrics-exporter:v1.2.0"
      selector_label_key: "feature.node.kubernetes.io/amd-gpu"
      selector_label_value: "true"

  # OPTIONAL: Pushgateway Manifest
  pushgateway_manifest:
    name: "pushgateway-setup"
    manifest_file: "files/pushgateway.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    variables:
      pushgateway_name: "pushgateway"
      pushgateway_namespace: "monitoring"
      pushgateway_replicas: 1
      pushgateway_image: "prom/pushgateway"
      pushgateway_image_pull_policy: "IfNotPresent"
      pushgateway_service_name: "pushgateway"
      pushgateway_service_type: "ClusterIP"
      pushgateway_service_protocol: "TCP"
      pushgateway_service_port: 9091
      pushgateway_service_target_port: 9091
      pushgateway_monitor_name: "pushgateway"
      pushgateway_monitor_release: "prometheus"
      pushgateway_monitor_path: "/metrics"
      pushgateway_monitor_interval: "5s"
      pushgateway_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

  smart_scaler_mcp_server_manifest:
    name: "smart-scaler-mcp-server-setup"
    manifest_file: "files/smart-scaler-mcp-server.yaml.j2"
    namespace: "smart-scaler"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      mcp_server_name: "smart-scaler-mcp-server"
      mcp_server_namespace: "smart-scaler"
      mcp_server_image: "docker.io/aveshasystems/smart-scaler-mcp-server:v1.0.1"
      mcp_server_image_pull_secret: "avesha-systems"
      mcp_server_prometheus_url: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090/"
      mcp_server_tenant_id: "tenant-b200-local"
      mcp_server_port: "8000"
      mcp_server_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

###############################################################################
# COMMAND EXECUTION CONFIGURATION
###############################################################################

command_exec:

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi

          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "fetch_worker_secret_worker_1"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # Create output directory if it doesn't exist
          mkdir -p output
          
          # Extract and encode values from the secret to create the values file
          echo "Generating worker values file..."
          echo "controllerSecret:" > output/worker-1-values.yaml
          
          # Extract namespace
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.namespace}' | \
            echo "  namespace: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract endpoint
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.controllerEndpoint}' | \
            echo "  endpoint: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract ca.crt
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.ca\.crt}' | \
            echo "  ca.crt: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            echo "  token: $(cat -)" >> output/worker-1-values.yaml
          
          echo "" >> output/worker-1-values.yaml
          echo "egsAgent:" >> output/worker-1-values.yaml
          echo "  secretName: egs-agent-access" >> output/worker-1-values.yaml
          echo "  agentSecret:" >> output/worker-1-values.yaml
          
          # Get UI proxy service endpoint
          UI_PROXY_PORT=$(kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get svc kubeslice-ui-proxy -n kubeslice-controller -o jsonpath='{.spec.ports[0].port}')
          
          echo "    endpoint: http://kubeslice-ui-proxy.kubeslice-controller.svc.cluster.local:${UI_PROXY_PORT}" >> output/worker-1-values.yaml
          
          # Get and decode the admin token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-rw-admin -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            base64 -d | \
            echo "    key: $(cat -)" >> output/worker-1-values.yaml
          
          echo "Worker values file generated at output/worker-1-values.yaml"
