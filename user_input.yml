---
################################################################################
# USER INPUT CONFIGURATION
# Last updated: JUNE 3, 2025
################################################################################

###############################################################################
# KUBERNETES DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new Kubernetes cluster using Kubespray
###############################################################################
kubernetes_deployment:
  # Master switch for Kubernetes deployment
  enabled: false                           # Enable kubernetes deployment
  
  # API Server Configuration
  # Defines how clients will connect to the Kubernetes API server
  api_server:
    host: "PUBLIC_IP"  # Reference global IP
    port: 6443              # The port for the Kubernetes API server
                           # 6443 is the default secure port for Kubernetes
    secure: true           # Whether to use HTTPS for API server connections
                          # Should always be true in production environments

  # SSH Access Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/absolute/path/to/.ssh/k8s_rsa"  # Full path to the SSH private key file
                                            # Used to authenticate with the cluster nodes
  default_ansible_user: "REPLACE_SSH_USER"         # Default SSH user for all nodes
                                           # Common values: ubuntu (Ubuntu), ec2-user (AWS)
  # Ansible Sudo Password Configuration
  # Optional: Can be set here, via ANSIBLE_SUDO_PASS environment variable,
  # or will prompt if neither is set
  ansible_sudo_pass: ""  # Leave empty to use environment variable or prompt

  # Control Plane Node Configuration
  # Defines the master nodes that will run the Kubernetes control plane
  control_plane_nodes:
    - name: "master-1"                # Unique identifier for the control plane node
      ansible_host: "PUBLIC_IP"  # Reference global IP
      ansible_user: "REPLACE_SSH_USER"      # SSH username for this specific node
                                     # Overrides default_ansible_user if different
      ansible_become: true            # Enable privilege escalation (sudo)
      ansible_become_method: "sudo"     # Method for privilege escalation
      ansible_become_user: "root"       # Target user for privilege escalation
      private_ip: "PRIVATE_IP"      # Internal/private IP address
                                     # Used for internal cluster communication
  # Kubernetes Network Configuration
  # Defines the network ranges for various Kubernetes components
  network_config:
    service_subnet: "10.233.0.0/18"  # CIDR range for Kubernetes services
                                    # Must not overlap with pod_subnet or node network
    pod_subnet: "10.233.64.0/18"    # CIDR range for Kubernetes pods
                                   # Must not overlap with service_subnet or node network
    node_prefix: 24                 # Subnet mask for node network
                                  # /24 allows 256 IP addresses per subnet

  # Firewall Configuration
  # Controls access to cluster nodes
  firewall:
    enabled: true                            # Whether to configure firewall rules
    allow_additional_ports:
      - "80"      # HTTP
      - "443"     # HTTPS
      - "6443"    # Kubernetes API server
      - "2379"    # etcd client API
      - "2380"    # etcd server API
      - "10250"   # Kubelet API
      - "10251"   # kube-scheduler
      - "10252"   # kube-controller-manager

  # NVIDIA Container Runtime Configuration
  # Required for GPU support in the cluster
  nvidia_runtime:
    enabled: true                           # Enable NVIDIA container runtime
                                          # Set to true if using NVIDIA GPUs
    install_toolkit: true                   # Install NVIDIA Container Toolkit
                                          # Required for GPU support
    configure_containerd: true              # Configure containerd for NVIDIA runtime
    create_runtime_class: true              # Create Kubernetes RuntimeClass for NVIDIA

  # Kubernetes Component Configuration
  # Core Kubernetes infrastructure choices
  network_plugin: "calico"                  # Container Network Interface (CNI) choice
                                          # Options: calico, flannel, weave, cilium
  
  container_runtime: "containerd"           # Container runtime for Kubernetes
                                          # containerd is the current standard
  
  dns_mode: "coredns"                      # DNS service for the cluster
                                         # CoreDNS is the default since Kubernetes 1.13
  # Async Execution Configuration
  # Controls timeout and polling for long-running tasks
  async_config:
    timeout: 3600                          # Maximum time (in seconds) to wait for task completion
    poll_interval: 5                       # How often (in seconds) to check task status

###############################################################################
# REQUIRED SETTINGS
# These settings must be configured regardless of deployment type
###############################################################################

# Validation and Execution Settings
validate_prerequisites:
  enabled: true                              # Required: Must be enabled for validation

execution_order_enabled: true                # Required: Must be enabled for ordered execution

# Required Kubeconfig Settings
global_control_plane_ip: "PUBLIC_IP"         # Provide the public IP for metallb/Nginx
global_kubeconfig: "output/kubeconfig"        # Required: Path to kubeconfig file
global_kubecontext: "kubernetes-admin@cluster.local"  # Required: Kubernetes context
use_global_context: true                     # Required: Use global context

# Required Helm Settings
use_local_charts: false                      # Required: Use local Helm charts
local_charts_path: "charts"                  # Required: Path to local charts
global_chart_repo_url: ""                    # Required: Global Helm repository URL
global_repo_username: ""                     # Required: Repo username if using private repos
global_repo_password: ""                     # Required: Repo password if using private repos
readd_helm_repos: true                       # Required: Re-add Helm repos

# Required Credentials
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') }}"              # Required for NGC access
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') }}" # Required for NGC Docker registry
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') }}" # Required for Avesha images
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') }}" # Required for Avesha images

# Required Execution Order
execution_order:
  # Core Infrastructure
  # - metallb_chart                 
  # - metallb_l2_config            
  # - metallb_ip_pool              
  # - nginx_ingress_config         
  # - nginx_ingress_chart  
  
  # Base Existing Applications
  - gpu_operator_chart
  - prometheus_stack
  - pushgateway_manifest
  - keda_chart
  - nim_operator_chart        # This creates the nim namespace
  - create_avesha_secret     # Move this before NGC secrets
  - create_ngc_secrets       # These need the nim namespace
  - verify_ngc_secrets

  # EGS Application
  - kubeslice_controller_egs  # Install EGS controller first
  - kubeslice_ui_egs         # Install EGS UI after controller
  - egs_project_manifest              # Create EGS project
  - egs_cluster_registration_worker_1 # Register worker-1 cluster
  - fetch_worker_secret_worker_1 # Fetch worker secret after registration
  - kubeslice_worker_egs_worker_1 # Install worker after fetching secret


  # # NIM Components
  - nim_cache_manifest_70b
  - wait_for_nim_cache_70b
  - nim_cache_wait_job_70b
  - nim_service_manifest_70b
  - keda_scaled_object_manifest_70b
  - create_inference_pod_configmap_70b
  - smart_scaler_inference_70b
  - create_locust_configmap_70b
  - locust_manifest_70b
  - smart_scaler_mcp_server_manifest

  #   # NIM 1B Components
  # - nim_cache_manifest_1b
  # - nim_service_manifest_1b
  # - keda_scaled_object_manifest_1b
  # - create_inference_pod_configmap_1b
  # - smart_scaler_inference_1b
  # - create_locust_configmap_1b
  # - locust_manifest_1b

  # # NIM 8B Components
  # - nim_cache_manifest_8b
  # - nim_service_manifest_8b
  # - keda_scaled_object_manifest_8b
  # - create_inference_pod_configmap_8b
  # - smart_scaler_inference_8b
  # - create_locust_configmap_8b
  # - locust_manifest_8b

###############################################################################
# OPTIONAL CONFIGURATION
###############################################################################

# MetalLB Configuration
metallb:
  enabled: false                      # Enable MetalLB
  namespace: "metallb"                   # Namespace for MetalLB deployment
  address_range: 172.18.1.1-172.18.1.16  # Reference global IP
  protocol: "layer2"                     # Use Layer 2 mode

# Nginx Ingress Configuration
nginx_ingress:
  enabled: false
  namespace: "ingress-nginx"
  replica_count: 1
  service_type: "LoadBalancer"
  metrics_enabled: true
  default_ssl_certificate: false
  enable_proxy_protocol: false
  enable_real_ip: true

# Optional Docker Registry Configuration
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"
  username: "{{ avesha_docker_username }}"
  password: "{{ avesha_docker_password }}"

###############################################################################
# HELM CHARTS CONFIGURATION
###############################################################################

helm_charts:

  # EGS Controller Chart Configuration
  kubeslice_controller_egs:
    release_name: "egs-controller"
    chart_ref: "kubeslice-controller-egs"
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://smartscaler.nexus.aveshalabs.io/repository/kubeslice-egs-helm-ent-prod"
    chart_version: "1.14.0"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
        namespaceConfig:
          labels: {}
          annotations: {}
        kubeTally:
          enabled: false
          postgresSecretName: kubetally-db-credentials
          postgresAddr: "kt-postgresql.kt-postgresql.svc.cluster.local"
          postgresPort: 5432
          postgresUser: "postgres"
          postgresPassword: "postgres"
          postgresDB: "postgres"
          postgresSslmode: disable
          prometheusUrl: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      kubeslice:
        controller:
          endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # EGS UI Chart Configuration
  kubeslice_ui_egs:
    release_name: "egs-ui"
    chart_ref: "kubeslice-ui-egs"
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://smartscaler.nexus.aveshalabs.io/repository/kubeslice-egs-helm-ent-prod"
    chart_version: "1.14.0"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
      kubeslice:
        prometheus:
          url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
        uiproxy:
          service:
            type: ClusterIP
          labels:
            app: kubeslice-ui-proxy
          annotations: {}
          ingress:
            enabled: false
            servicePort: 443
            className: ""
            hosts:
              - host: ui.kubeslice.com
                paths:
                  - path: /
                    pathType: Prefix
            tls: []
            annotations: []
            extraLabels: {}
        egsCoreApis:
          enabled: true
          service:
            type: ClusterIP
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # MetalLB Chart Configuration

  metallb_chart:
    release_name: "metallb"
    chart_ref: "metallb"
    release_namespace: "{{ metallb.namespace }}"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://metallb.github.io/metallb"
    chart_version: "0.13.10"
    release_values:
      prometheus:
        serviceMonitor:
          enabled: false
      controller:
        logLevel: info
      speaker:
        logLevel: info
        frr:
          enabled: false
      webhookConfiguration:
        enabled: true
        timeoutSeconds: 15
      crds:
        enabled: true
      rbac:
        create: true

  # Nginx Ingress Controller Chart
  nginx_ingress_chart:
    release_name: "ingress-nginx"
    chart_ref: "ingress-nginx"
    release_namespace: "ingress-nginx"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://kubernetes.github.io/ingress-nginx"
    chart_repo_name: "ingress-nginx"
    chart_version: "4.7.1"
    release_values:
      controller:
        replicaCount: 1
        service:
          enabled: true
          type: "NodePort"
        metrics:
          enabled: true
          serviceMonitor:
            enabled: false
        config:
          use-proxy-protocol: false
          real-ip-header: "proxy_protocol"
          proxy-real-ip-cidr: "0.0.0.0/0"
          use-forwarded-headers: "true"
        publishService:
          enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
            timeoutSeconds: 15
        resources:
          requests:
            cpu: "1000m"
            memory: "1024Mi"
          limits:
            cpu: "2000m"
            memory: "2048Mi"
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
      defaultBackend:
        enabled: false
        service:
          type: NodePort
        resources:
          requests:
            cpu: "1000m"
            memory: "2048"
          limits:
            cpu: "2000m"
            memory: "4096Mi"

  gpu_operator_chart:
    release_name: "gpu-operator"
    chart_ref: "gpu-operator"
    release_namespace: "gpu-operator"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_version: "v25.3.0"
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    dependencies: []
    release_values:
      mig:
        strategy: "none"
      dcgm:
        enabled: true
      driver:
        enabled: false
      cdi:
        default: true
        enabled: true
      hostPaths:
        driverInstallDir: "/home/kubernetes/bin/nvidia"
      toolkit:
        installDir: "/home/kubernetes/bin/nvidia"
        env:
          - name: "CONTAINERD_RUNTIME_CLASS"
            value: "nvidia"

  # Prometheus Stack
  prometheus_stack:
    release_name: "prometheus"
    chart_ref: "kube-prometheus-stack"
    release_namespace: "monitoring"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: false
    atomic: false
    skip_diff: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false
      prometheus:
        service:
          type: NodePort
        prometheusSpec:
          retention: "15d"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: local-path
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 10Gi
          additionalScrapeConfigs:
            - job_name: "gpu-metrics"
              scrape_interval: "1s"
              metrics_path: "/metrics"
              scheme: "http"
              kubernetes_sd_configs:
                - role: "endpoints"
                  namespaces:
                    names:
                      - "monitoring"
                      - "gpu-operator"
              relabel_configs:
                - source_labels: ["__meta_kubernetes_endpoints_name"]
                  action: "drop"
                  regex: ".*-node-feature-discovery-master"
                - source_labels: ["__meta_kubernetes_pod_node_name"]
                  action: "replace"
                  target_label: "kubernetes_node"
      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
      kubelet:
        serviceMonitor:
          https: false
      grafana:
        enabled: true
        persistence:
          enabled: true
          storageClassName: local-path
          size: 1Gi
          accessModes: ["ReadWriteOnce"]
        service:
          type: NodePort

  # KEDA Chart
  keda_chart:
    release_name: "keda"
    chart_ref: "keda"
    release_namespace: "keda"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_repo_url: "https://kedacore.github.io/charts"
    chart_version: "2.12.1"
    dependencies: []

  # NIM Operator Chart
  nim_operator_chart:
    release_name: "nim"
    chart_ref: "k8s-nim-operator"
    release_namespace: "nim"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    chart_version: "v1.0.1"
    dependencies: []

  # Kubeslice Worker Chart Configuration
  kubeslice_worker_egs_worker_1:
    release_name: "egs-worker"
    chart_ref: "kubeslice-worker-egs"
    release_namespace: "kubeslice-system"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://smartscaler.nexus.aveshalabs.io/repository/kubeslice-egs-helm-ent-prod"
    chart_version: "1.14.0"
    values_files:
      - "output/worker-1-values.yaml"
    release_values:
      cluster:
        name: "worker-1"
        endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      global:
        imageRegistry: "docker.io/aveshasystems"
      egs:
        prometheusEndpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        grafanaDashboardBaseUrl: "http://<grafana-lb>/d/Oxed_c6Wz"
      metrics:
        insecure: true
      kserve:
        enabled: true
        kserve:
          controller:
            gateway:
              domain: "kubeslice.com"
              ingressGateway:
                className: "nginx"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

###############################################################################
# KUBERNETES MANIFESTS CONFIGURATION
###############################################################################

manifests:

  # EGS Project Configuration
  egs_project_manifest:
    name: "egs-project"
    manifest_file: "files/egs-project.yaml.j2"
    namespace: "kubeslice-controller"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - kubeslice_controller_egs
    variables:
      project_name: "avesha"
      project_namespace: "kubeslice-controller"
      admin_user: "admin"

  # EGS Cluster Registration Configuration
  egs_cluster_registration_worker_1:
    name: "egs-cluster-registration-worker-1"
    manifest_file: "files/egs-cluster-registration.yaml.j2"
    namespace: "kubeslice-avesha"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - egs_project_manifest
    variables:
      cluster_name: "worker-1"
      project_name: "avesha"
      telemetry:
        enabled: true
        endpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        telemetryProvider: "prometheus"
      geoLocation:
        cloudProvider: ""
        cloudRegion: ""

  # MetalLB IP Pool Configuration
  metallb_ip_pool:
    name: "metallb-ip-pool"
    manifest_file: "files/metallb-ip-pool.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      metallb_pool_name: "first-pool"
      metallb_namespace: "metallb"
      metallb_address_range: "172.18.1.1-172.18.1.16"  # Using the control plane IP

  # MetalLB L2 Advertisement Configuration
  metallb_l2_config:
    name: "metallb-l2-config"
    manifest_file: "files/metallb-l2-config.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: 
      - metallb_chart  # Ensure MetalLB chart is fully installed first
    variables:
      metallb_l2_name: "l2"
      metallb_namespace: "metallb"
      metallb_pool_name: "first-pool"

  # Nginx Ingress Additional Configuration
  nginx_ingress_config:
    name: "nginx-ingress-config"
    manifest_file: "files/nginx-ingress-config.yaml.j2"
    namespace: "ingress-nginx"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: []
    variables:
      nginx_config_name: "nginx-ingress-controller-custom-config"
      nginx_namespace: "ingress-nginx"
      nginx_proxy_buffer_size: "128k"
      nginx_proxy_body_size: "50m"
      nginx_client_header_buffer_size: "64k"
      nginx_large_client_header_buffers: "4 128k"
      nginx_client_body_buffer_size: "128k"
      nginx_enable_brotli: "true"
      nginx_use_gzip: "true"
      nginx_gzip_level: "6"
      nginx_gzip_types: "application/atom+xml application/javascript application/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/javascript text/plain text/x-component"

  # OPTIONAL: Pushgateway Manifest
  pushgateway_manifest:
    name: "pushgateway-setup"
    manifest_file: "files/pushgateway.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    variables:
      pushgateway_name: "pushgateway"
      pushgateway_namespace: "monitoring"
      pushgateway_replicas: 1
      pushgateway_image: "prom/pushgateway"
      pushgateway_image_pull_policy: "IfNotPresent"
      pushgateway_service_name: "pushgateway"
      pushgateway_service_type: "ClusterIP"
      pushgateway_service_protocol: "TCP"
      pushgateway_service_port: 9091
      pushgateway_service_target_port: 9091
      pushgateway_monitor_name: "pushgateway"
      pushgateway_monitor_release: "prometheus"
      pushgateway_monitor_path: "/metrics"
      pushgateway_monitor_interval: "5s"
      pushgateway_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

 # NIM 1B Components
  nim_cache_manifest_1b:
    name: "nim-cache-setup-1b"
    manifest_file: "files/nim-cache-1b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-1b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.2-1b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_1b:
    name: "nim-service-setup-1b"
    manifest_file: "files/nim-service-1b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-1b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.2-1b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-1b-instruct"
      nim_service_storage_cache_profile: "" #TODO: change to 1b profile
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_1b:
    name: "keda-scaled-object-setup-1b"
    manifest_file: "files/keda-scaled-object-1b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-1b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-1b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: >-
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-1b-instruct"}

  smart_scaler_inference_1b:
    name: "smart-scaler-inference-setup-1b"
    manifest_file: "files/smart-scaler-inference-1b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-1b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-1b"
        cluster_name: "nim-llama-1b"
        tenant_id: "tenant-b200-local-1b"
        app_name: "nim-llama-1b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-1b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_1b:
    name: "locust-load-1b"
    manifest_file: "files/locust-deploy-1b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-1b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-1b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-1b"

 # NIM 1B Components
  nim_cache_manifest_8b:
    name: "nim-cache-setup-8b"
    manifest_file: "files/nim-cache-8b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-8b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.4"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "vllm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_8b:
    name: "nim-service-setup-8b"
    manifest_file: "files/nim-service-8b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-8b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.4"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-8b-instruct"
      nim_service_storage_cache_profile: "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_8b:
    name: "keda-scaled-object-setup-8b"
    manifest_file: "files/keda-scaled-object-8b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-8b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-8b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-8b-instruct"}

  smart_scaler_inference_8b:
    name: "smart-scaler-inference-setup-8b"
    manifest_file: "files/smart-scaler-inference-8b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-8b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-8b"
        cluster_name: "nim-llama-8b"
        tenant_id: "tenant-b200-local-8b"
        app_name: "nim-llama-8b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-8b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_8b:
    name: "locust-load-8b"
    manifest_file: "files/locust-deploy-8b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-8b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-8b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-8b"


   # OPTIONAL: NIM Cache Manifest
  nim_cache_manifest_70b:
    name: "nim-cache-setup-70b"
    manifest_file: "files/nim-cache-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-70b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-70b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}


  nim_cache_wait_job_70b:
    name: "wait-for-nimcache-70b"
    manifest_file: "files/wait-job-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 1200
    wait_condition:
      type: "Succeeded"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      wait_job_name: "wait-for-nimcache-70b"
      wait_job_namespace: "nim"
      wait_job_target_cache: "meta-llama3-70b-instruct"
      wait_job_timeout: 1200
      wait_job_check_interval: 10
      wait_job_service_account: "nim-job-reader"

  nim_service_manifest_70b:
    name: "nim-service-setup-70b"
    manifest_file: "files/nim-service-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-70b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-70b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-70b-instruct"
      nim_service_storage_cache_profile: "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_70b:
    name: "keda-scaled-object-setup-70b"
    manifest_file: "files/keda-scaled-object-70b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-70b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-70b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-70b-instruct"}

  smart_scaler_inference_70b:
    name: "smart-scaler-inference-setup-70b"
    manifest_file: "files/smart-scaler-inference-70b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-70b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-70b"
        cluster_name: "nim-llama"
        tenant_id: "tenant-b200-local"
        app_name: "nim-llama"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-70b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_70b:
    name: "locust-load-70b"
    manifest_file: "files/locust-deploy-70b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-70b"
      locust_namespace: "nim-load-test"
      locust_replicas: 0
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-70b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-70b"

  smart_scaler_mcp_server_manifest:
    name: "smart-scaler-mcp-server-setup"
    manifest_file: "files/smart-scaler-mcp-server.yaml.j2"
    namespace: "smart-scaler"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      mcp_server_name: "smart-scaler-mcp-server"
      mcp_server_namespace: "smart-scaler"
      mcp_server_image: "docker.io/aveshasystems/smart-scaler-mcp-server:v1.0.0"
      mcp_server_image_pull_secret: "avesha-systems"
      mcp_server_prometheus_url: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090/"
      mcp_server_tenant_id: "tenant-b200-local"
      mcp_server_port: "8000"
      mcp_server_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

###############################################################################
# COMMAND EXECUTION CONFIGURATION
###############################################################################

command_exec:
  - name: "wait_for_nim_cache_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Waiting for Job wait-for-nimcache-70b to complete..."
          start_time=$(date +%s)
          timeout=1200
          job_complete=false
          while [ $(($(date +%s) - start_time)) -lt $timeout ]; do
            if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
               --context={{ kubecontext | default(global_kubecontext) }} \
               -n nim get job wait-for-nimcache-70b -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' | grep -q "True"; then
              job_complete=true
              break
            fi
            echo "Job not complete yet, waiting... ($(($timeout - ($(date +%s) - start_time))) seconds remaining)"
            # Show job status
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim get job wait-for-nimcache-70b -o jsonpath='{.status.conditions[*]}' || true
            echo
            sleep 10
          done
          if [ "$job_complete" = true ]; then
            echo "Job completed successfully!"
          else
            echo "WARNING: Timeout waiting for Job"
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim describe job wait-for-nimcache-70b || true
          fi
          # Don't exit with error, let Ansible continue
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: true
        
  - name: "fetch_worker_secret_worker_1"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # Create output directory if it doesn't exist
          mkdir -p output
          
          # Extract and encode values from the secret to create the values file
          echo "Generating worker values file..."
          echo "controllerSecret:" > output/worker-1-values.yaml
          
          # Extract namespace
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.namespace}' | \
            echo "  namespace: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract endpoint
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.controllerEndpoint}' | \
            echo "  endpoint: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract ca.crt
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.ca\.crt}' | \
            echo "  ca.crt: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            echo "  token: $(cat -)" >> output/worker-1-values.yaml
          
          echo "" >> output/worker-1-values.yaml
          echo "egsAgent:" >> output/worker-1-values.yaml
          echo "  secretName: egs-agent-access" >> output/worker-1-values.yaml
          echo "  agentSecret:" >> output/worker-1-values.yaml
          
          # Get UI proxy service endpoint
          UI_PROXY_PORT=$(kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get svc kubeslice-ui-proxy -n kubeslice-controller -o jsonpath='{.spec.ports[0].port}')
          
          echo "    endpoint: http://kubeslice-ui-proxy.kubeslice-controller.svc.cluster.local:${UI_PROXY_PORT}" >> output/worker-1-values.yaml
          
          # Get and decode the admin token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-rw-admin -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            base64 -d | \
            echo "    key: $(cat -)" >> output/worker-1-values.yaml
          
          echo "Worker values file generated at output/worker-1-values.yaml"

  - name: "create_ngc_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      # First check if secrets exist
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim get secret ngc-secret >/dev/null 2>&1; then
            echo "Secret ngc-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim delete secret ngc-secret
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim create secret docker-registry ngc-secret \
            --docker-server=nvcr.io \
            --docker-username='$oauthtoken' \
            --docker-password="${NGC_DOCKER_API_KEY}" \
            --docker-email='your.email@solo.io'
        env:
          NGC_DOCKER_API_KEY: "{{ ngc_docker_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

      # Handle ngc-api-secret
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret ngc-api-secret -n nim >/dev/null 2>&1; then
            echo "Secret ngc-api-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              delete secret ngc-api-secret -n nim
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create secret generic ngc-api-secret -n nim\
            --from-literal=NGC_API_KEY="${NGC_API_KEY}"
        env:
          NGC_API_KEY: "{{ ngc_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_ngc_secrets"
    commands:
      - cmd: "kubectl get secret ngc-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true
      - cmd: "kubectl get secret ngc-api-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi

          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true


  - name: "create_inference_pod_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-1b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-1b --from-file="config.json"="files/config-inference-1b.json"

  - name: "create_inference_pod_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-8b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-8b --from-file="config.json"="files/config-inference-8b.json"

  - name: "create_inference_pod_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-70b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-70b --from-file="config.json"="files/config-inference-70b.json"

  - name: "create_locust_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-1b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-1b --from-file="locustfile.py"="files/locust-1b.py"

  - name: "create_locust_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-8b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-8b --from-file="locustfile.py"="files/locust-8b.py"

  - name: "create_locust_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-70b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-70b --from-file="locustfile.py"="files/locust-70b.py"
