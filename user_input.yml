---
################################################################################
# USER INPUT CONFIGURATION
# Last updated: AUG 15, 2025
################################################################################

###############################################################################
# KUBERNETES DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new Kubernetes cluster using Kubespray
###############################################################################
kubernetes_deployment:
  # Master switch for Kubernetes deployment
  enabled: false                           # Enable kubernetes deployment
  
  # API Server Configuration
  # Defines how clients will connect to the Kubernetes API server
  api_server:
    host: "PUBLIC_IP"        # Load balancer VIP or first master for initial setup
    port: 6443                   # Standard Kubernetes API port (will be load balanced)
                                # Standard Kubernetes API port - change only if required
    bind_address: "0.0.0.0"      # Listen on all interfaces
    secure: true                 # Whether to use HTTPS for API server connections
                                # Should always be true in production environments

  # SSH Access Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/absolute/path/to/.ssh/k8s_rsa"  # Full path to the SSH private key file
                                            # Used to authenticate with the cluster nodes
  default_ansible_user: "REPLACE_SSH_USER"         # Default SSH user for all nodes
                                           # Common values: ubuntu (Ubuntu), ec2-user (AWS)
  # Ansible Sudo Password Configuration
  # Optional: Can be set here, via ANSIBLE_SUDO_PASS environment variable,
  # or will prompt if neither is set
  ansible_sudo_pass: ""  # Leave empty to use environment variable or prompt


  # Control Plane Node Configuration (Multi-Master HA Support)
  # Minimum 3 nodes for HA, always use odd numbers (3, 5, 7)
  control_plane_nodes:
    - name: "master-1"                # Unique identifier for the control plane node
      ansible_host: "PUBLIC_IP"  # Replace with actual master-1 public IP
      ansible_user: "root"      # SSH username for this specific node
                                     # Overrides default_ansible_user if different
      ansible_become: true            # Enable privilege escalation (sudo)
      ansible_become_method: "sudo"     # Method for privilege escalation
      ansible_become_user: "root"       # Target user for privilege escalation
      private_ip: "PRIVATE_IP"      # Replace with actual master-1 private IP
                                     # Used for internal cluster communication
    # # Add more masters for HA (uncomment and configure):
    # - name: "master-2"
    #   ansible_host: "PUBLIC_IP_2"
    #   ansible_user: "root"
    #   ansible_become: true
    #   ansible_become_method: "sudo"
    #   ansible_become_user: "root"
    #   private_ip: "PRIVATE_IP_2"
    # - name: "master-3"
    #   ansible_host: "PUBLIC_IP_3"
    #   ansible_user: "root"
    #   ansible_become: true
    #   ansible_become_method: "sudo"
    #   ansible_become_user: "root"
    #   private_ip: "PRIVATE_IP_3"

  # Worker Node Configuration (Optional but recommended)
  # worker_nodes: []
  # Example worker node configuration:
  # worker_nodes:
  #   - name: "worker-1"
  #     ansible_host: "WORKER_PUBLIC_IP"
  #     ansible_user: "root"
  #     ansible_become: true
  #     ansible_become_method: "sudo"
  #     ansible_become_user: "root"
  #     private_ip: "WORKER_PRIVATE_IP"



      # Python Configuration
  python_config:
    interpreter_path: "/usr/bin/python3"  # Path to Python interpreter
                                     # Used for internal cluster communication
  # Kubernetes Network Configuration
  # Defines the network ranges for various Kubernetes components
  network_config:
    service_subnet: "10.233.0.0/18"  # CIDR range for Kubernetes services
                                    # Must not overlap with pod_subnet or node network
    pod_subnet: "10.233.64.0/18"    # CIDR range for Kubernetes pods
                                   # Must not overlap with service_subnet or node network
    node_prefix: 24                 # Subnet mask for node network
                                  # /24 allows 256 IP addresses per subnet

  # Firewall Configuration
  # Controls access to cluster nodes
  firewall:
    enabled: false                            # Whether to configure firewall rules
    allow_additional_ports:
      - "80"      # HTTP
      - "443"     # HTTPS
      - "6443"    # Kubernetes API server
      - "2379"    # etcd client API
      - "2380"    # etcd server API
      - "10250"   # Kubelet API
      - "10251"   # kube-scheduler
      - "10252"   # kube-controller-manager

  # NVIDIA Container Runtime Configuration
  # Required for GPU support in the cluster
  nvidia_runtime:
    enabled: true                           # Enable NVIDIA container runtime
                                          # Set to true if using NVIDIA GPUs
    install_toolkit: true                   # Install NVIDIA Container Toolkit
                                          # Required for GPU support
    configure_containerd: true              # Configure containerd for NVIDIA runtime
    create_runtime_class: true              # Create Kubernetes RuntimeClass for NVIDIA
    architecture: "amd64"                   # Architecture for NVIDIA container runtime package

  # Kubernetes Component Configuration
  # Core Kubernetes infrastructure choices
  network_plugin: "calico"                  # Container Network Interface (CNI) choice
                                          # Options: calico, flannel, weave, cilium
  
  container_runtime: "containerd"           # Container runtime for Kubernetes
                                          # containerd is the current standard
  
  dns_mode: "coredns"                      # DNS service for the cluster
                                         # CoreDNS is the default since Kubernetes 1.13
                                         
  # Kubelet Configuration
  # Controls node-level settings for the Kubernetes node agent
  kubelet_config:
    max_pods: 256                          # Maximum number of pods per node
                                         # Default is 110, can be increased for high-density nodes
                                         # Consider network and resource constraints when increasing
  # Async Execution Configuration
  # Controls timeout and polling for long-running tasks
  async_config:
    timeout: 3600                          # Maximum time (in seconds) to wait for task completion
    poll_interval: 5                       # How often (in seconds) to check task status

  # Load Balancer Configuration
  # ============================================================================
  # DEFAULT STRATEGY: Load Balancer ALWAYS ENABLED
  # ============================================================================
  # WHY: Future-proof scaling, health monitoring, operational consistency
  # WORKS: Single master (1 node) and multi-master (3+ nodes) deployments
  # FLOW: kubectl → haproxy:6443 → API server:6444 (on each master)
  # ============================================================================
  load_balancer:
    enabled: true                          # DEFAULT: Always enabled (recommended best practice)
    type: "localhost"                      # DEFAULT: Use localhost haproxy proxy
    
    # Localhost Load Balancer Configuration (DEFAULT: Enabled for all deployments)
    # Benefits: Future-proof scaling, health monitoring, consistent architecture
    localhost:
      enabled: true                        # DEFAULT: Always enabled (works for single/multi-master)
      lb_type: "haproxy"                   # Using HAProxy for enterprise-grade load balancing
      bind_address: "0.0.0.0"             # DEFAULT: Bind to all interfaces
      api_port_offset: 1                  # DEFAULT: API server port = client_port + offset (6443+1=6444)
      pod_name: "haproxy"                 # HAProxy load balancer pod name
      healthcheck_port: 8081              # DEFAULT: Health monitoring port
      keepalive_timeout: "5m"             # DEFAULT: Connection timeout
      memory_requests: "32M"              # DEFAULT: Memory allocation for LB pod
      cpu_requests: "25m"                 # DEFAULT: CPU allocation for LB pod
    
    # # External Load Balancer Configuration (ALTERNATIVE: For external LB like HAProxy/F5)
    # external:
    #   enabled: false                       # ALTERNATIVE: Use only if you have external load balancer
    #   address: ""                         # External LB IP address (defaults to api_server.host)
    #   domain_name: "k8s-api.yourdomain.com"  # Optional: Domain name for LB
    #                                       # External LB port uses api_server.port automatically
    
    # # kube-vip Configuration (ALTERNATIVE: For VIP-based HA)
    # kube_vip:
    #   enabled: false                       # ALTERNATIVE: Use only for VIP-based high availability
    #   controlplane_enabled: true          # Enable kube-vip for control plane
    #   vip_address: ""                     # Virtual IP address (defaults to api_server.host)
    #   interface: "eth0"                   # Network interface for VIP
    #   arp_enabled: true                   # Enable ARP for VIP management
    #   leader_election: true               # Enable leader election
    #   cidr: 24                           # CIDR for VIP network

  # etcd High Availability Configuration
  # Critical for multi-master cluster stability and data consistency
  # NOTE: Disabled for single master setup
  etcd_ha:
    enabled: true                         # Disabled for single master setup
    deployment_type: "kubeadm"            # Deployment type: "kubeadm" or "external"
    
    # etcd Cluster Configuration
    cluster:
      initial_cluster_state: "new"        # Initial cluster state: "new" or "existing"
      heartbeat_interval: "100"           # Heartbeat interval in milliseconds
      election_timeout: "1000"            # Election timeout in milliseconds
      quota_backend_bytes: "2147483648"   # Storage quota (2GB default)
      auto_compaction_retention: "8"      # Auto compaction retention in hours
      max_request_bytes: "1572864"        # Max request size (1.5MB)
      metrics: "basic"                    # Metrics level: "basic" or "extensive"
    
    # etcd Events Cluster (Optional)
    events_cluster:
      enabled: false                      # Disabled for single master
      setup: false                       # Disabled for single master
    
    # etcd Security Configuration
    security:
      peer_auto_tls: false               # Disable peer auto TLS (use proper certs)
      client_cert_auth: true             # Require client certificate authentication
      peer_cert_auth: true               # Require peer certificate authentication
    
    # etcd Performance Tuning
    performance:
      max_snapshots: 5                   # Maximum number of snapshots to retain
      max_wals: 5                        # Maximum number of WAL files to retain
      snapshot_count: 100000             # Snapshot after this many transactions
      wal_dir: ""                        # Custom WAL directory (empty = default)
      data_dir: ""                       # Custom data directory (empty = default)

###############################################################################
# K3S DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new K3s cluster using k3s-ansible
# Alternative to kubespray for lightweight Kubernetes deployment
###############################################################################
k3s_deployment:
  # Master switch for K3s deployment
  enabled: true                              # Enable K3s deployment
  
  # K3s Version Configuration
  k3s_version: "v1.33.3+k3s1"                # K3s version to install
  
  # K3s Configuration
  k3s_config:
    # Network Configuration
    service_cidr: "10.43.0.0/16"             # CIDR range for K3s services
    cluster_cidr: "10.42.0.0/16"             # CIDR range for K3s pods
    cluster_dns: "10.43.0.10"                 # Cluster DNS service IP
    
    # CNI Configuration
    cni: "flannel"                            # CNI plugin (flannel, calico, cilium)
    
    # Additional K3s server arguments
    extra_server_args: ""                     # Additional arguments for K3s server
    
    # Additional K3s agent arguments
    extra_agent_args: ""                      # Additional arguments for K3s agents
    
    # External Database Configuration (for HA)
    use_external_database: false              # Use external database instead of embedded etcd
    datastore_endpoint: ""                    # External database endpoint (e.g., postgres://...)
    
    # Airgap Installation
    airgap_dir: ""                            # Path to airgap directory containing K3s binaries
    
    # SELinux Support
    selinux_enabled: false                    # Enable SELinux support
    
    # TLS Configuration
    tls_san: []                               # Additional TLS SANs for certificates
    
    # Node Labels
    node_labels: {}                            # Labels to apply to nodes
    
    # Node Taints
    node_taints: {}                            # Taints to apply to nodes





 
###############################################################################
# REQUIRED SETTINGS
# These settings must be configured regardless of deployment type
###############################################################################

skip_prerequisites: false

# Jetson Prerequisites Configuration
# Automatically detect Jetson devices and install jetson-stats for monitoring
# This works for BOTH K3s and Kubespray deployments
jetson_prerequisites:
  enabled: true                           # Enable Jetson detection and jetson-stats installation
                                          # Set to true to automatically detect and configure Jetson devices
  
  # jetson-stats installation options
  jetson_stats:
    force_reinstall: false                # Force reinstallation even if already present
    upgrade: true                         # Upgrade to latest version
    python_version: "python3"            # Python version to use (python3 is default)
    pip_extra_args: "--upgrade"          # Additional pip arguments
  
  # Detection methods to use
  detection_methods:
    - device_tree_model                   # Check /proc/device-tree/model
    - device_tree_compatible              # Check /proc/device-tree/compatible
    - nv_tegra_release                   # Check /etc/nv_tegra_release
    - system_architecture                 # Check system architecture
  
  # Timeout for jtop test (seconds)
  jtop_test_timeout: 5
  
  # Verbose output
  verbose: false

# NVIDIA Prerequisites Configuration
# Automatically detect NVIDIA GPUs and install container runtime support
# This works for BOTH K3s and Kubespray deployments
nvidia_prerequisites:
  enabled: true                           # Enable NVIDIA GPU detection and container runtime setup
                                          # Set to true if using NVIDIA GPUs in your cluster
  
  # NVIDIA Container Runtime Configuration
  container_runtime:
    install_toolkit: true                  # Install NVIDIA Container Toolkit
                                          # Required for GPU support in containers
    configure_containerd: true             # Configure containerd for NVIDIA runtime
    create_runtime_class: true             # Create Kubernetes RuntimeClass for NVIDIA
    architecture: "amd64"                  # Architecture for NVIDIA container runtime package
  
  # NVIDIA Driver Configuration
  driver:
    auto_install: false                    # Automatically install NVIDIA drivers (use with caution)
    version: "latest"                      # Driver version to install (latest, specific version, or auto)
    install_method: "package"              # Installation method: package, runfile, or container
  
  # GPU Detection and Validation
  detection:
    enabled: true                          # Enable automatic GPU detection
    methods:
      - nvidia_smi                         # Use nvidia-smi command
      - lspci                              # Use lspci to find NVIDIA devices
      - device_files                       # Check for /dev/nvidia* files
      - kernel_modules                     # Check for loaded nvidia kernel modules
  
  # Verification Settings
  verification:
    test_gpu_access: true                  # Test GPU access from containers
    create_test_pod: true                  # Create test pod to verify GPU functionality
    timeout: 300                           # Timeout for verification tests (seconds)
  
  # Advanced Settings
  advanced:
    enable_mps: false                      # Enable NVIDIA Multi-Process Service
    enable_uvm: true                       # Enable Unified Virtual Memory
    enable_persistent_mode: true           # Enable persistent mode for GPUs
    verbose: false                         # Verbose output during installation

# Validation and Execution Settings
validate_prerequisites:
  enabled: true                              # Required: Must be enabled for validation

execution_order_enabled: true                # Required: Must be enabled for ordered execution

# Required Kubeconfig Settings
global_control_plane_ip: "PUBLIC_IP"         # Provide the public IP for metallb/Nginx
global_kubeconfig: "output/kubeconfig"        # Required: Path to kubeconfig file
global_kubecontext: "default"  # Required: Kubernetes context
use_global_context: true                     # Required: Use global context

# Required Helm Settings
use_local_charts: true                      # Required: Use local Helm charts
local_charts_path: "files/charts"                  # Required: Path to local charts
global_chart_repo_url: ""                    # Required: Global Helm repository URL
global_repo_username: ""                     # Required: Repo username if using private repos
global_repo_password: ""                     # Required: Repo password if using private repos
readd_helm_repos: true                       # Required: Re-add Helm repos

# Required Credentials
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') }}"              # Required for NGC access
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') }}" # Required for NGC Docker registry
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') }}" # Required for Avesha images
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') }}" # Required for Avesha images

# Required Execution Order
execution_order:
  # Core Infrastructure
  # - metallb_chart                 
  # - metallb_l2_config            
  # - metallb_ip_pool              
  # - nginx_ingress_config         
  # - nginx_ingress_chart
  # - cert_manager 

  # Base Applications
  # - gpu_operator_chart
  # - prometheus_stack
  # - pushgateway_manifest
  # - keda_chart
  # - nim_operator_chart
  # - create_ngc_secrets
  # - verify_ngc_secrets
  # - create_avesha_secret

  # Jetson GPU Support
  - prometheus_stack
  - jetson_prerequisites
  - nvidia_device_plugin_manifest
  - jetson_stats_node_exporter_manifest
  - jetson_stats_node_exporter_service_manifest
  - jetson_stats_node_exporter_servicemonitor_manifest

  # AMD GPU Support
  # - amd_gpu_operator_chart         # Install AMD GPU operator for AMD GPU support,enable cert-manager as well from above section.
  # - amd_gpu_deviceconfig_manifest  # Configure AMD GPU device settings
  
  # SmartAI Infrastructure
  # - cert_manager                  # Install cert-manager first
  # - create_cert_manager_issuers   # Create cert-manager issuers and certificates
  # - create_smartai_namespaces_and_certificates  # Create smartai namespaces and certificates
  # - verify_certificates_and_create_ca_secrets  # Verify certificates and create CA secrets
  # - patch_nodelocaldns_config     # Patch nodelocaldns for custom DNS entries
  # - local_path_retain_storageclass  # Create local-path-retain StorageClass
  # - apply_knative_serving_crds    # Install Knative serving CRDs via kubectl
  # - apply_knative_serving_core    # Install Knative serving core components via kubectl
  # - apply_knative_serving_hpa     # Install Knative serving HPA via kubectl
  # - apply_knative_monitor         # Install Knative monitor components via kubectl       

  # EGS Application
  # - kubeslice_controller_egs  # Install EGS controller first
  # - kubeslice_ui_egs         # Install EGS UI after controller
  # - egs_project_manifest              # Create EGS project
  # - egs_cluster_registration_worker_1 # Register worker-1 cluster
  # - fetch_worker_secret_worker_1 # Fetch worker secret after registration
  # - kubeslice_worker_egs_worker_1 # Install worker after fetching secret

  # NIM 70B Components
  # - nim_cache_manifest_70b
  # - wait_for_nim_cache_70b
  # - nim_cache_wait_job_70b
  # - nim_service_manifest_70b
  # - keda_scaled_object_manifest_70b
  # - create_inference_pod_configmap_70b
  # - smart_scaler_inference_70b
  # - create_locust_configmap_70b
  # - locust_manifest_70b
  # - smart_scaler_mcp_server_manifest

  #   # NIM 1B Components
  # - nim_cache_manifest_1b
  # - nim_service_manifest_1b
  # - keda_scaled_object_manifest_1b
  # - create_inference_pod_configmap_1b
  # - smart_scaler_inference_1b
  # - create_locust_configmap_1b
  # - locust_manifest_1b

  # # NIM 8B Components
  # - nim_cache_manifest_8b
  # - nim_service_manifest_8b
  # - keda_scaled_object_manifest_8b
  # - create_inference_pod_configmap_8b
  # - smart_scaler_inference_8b
  # - create_locust_configmap_8b
  # - locust_manifest_8b

###############################################################################
# OPTIONAL CONFIGURATION
###############################################################################

# MetalLB Configuration
metallb:
  enabled: false                      # Enable MetalLB
  namespace: "metallb"                   # Namespace for MetalLB deployment
  address_range: 172.18.1.1-172.18.1.16  # Reference global IP
  protocol: "layer2"                     # Use Layer 2 mode

# Nginx Ingress Configuration
nginx_ingress:
  enabled: false
  namespace: "ingress-nginx"
  replica_count: 1
  service_type: "LoadBalancer"
  metrics_enabled: true
  default_ssl_certificate: false
  enable_proxy_protocol: false
  enable_real_ip: true

# Optional Docker Registry Configuration
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"  # Optional: Docker registry URL
  username: "{{ avesha_docker_username }}"
  password: "{{ avesha_docker_password }}"

###############################################################################
# HELM CHARTS CONFIGURATION
###############################################################################

helm_charts:

  # Cert Manager Chart Configuration
  cert_manager:
    release_name: "cert-manager"
    chart_ref: "./cert-manager"
    release_namespace: "cert-manager"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "v1.13.3"
    release_values:
      installCRDs: true
      global:
        logLevel: 2
      prometheus:
        enabled: false
        servicemonitor:
          enabled: false
      webhook:
        timeoutSeconds: 30

  # EGS Controller Chart Configuration
  kubeslice_controller_egs:
    release_name: "egs-controller"
    chart_ref: "./kubeslice-controller-egs"    # Using relative path
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
        namespaceConfig:
          labels: {}
          annotations: {}
        kubeTally:
          enabled: false
          postgresSecretName: kubetally-db-credentials
          postgresAddr: "kt-postgresql.kt-postgresql.svc.cluster.local"
          postgresPort: 5432
          postgresUser: "postgres"
          postgresPassword: "postgres"
          postgresDB: "postgres"
          postgresSslmode: disable
          prometheusUrl: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      kubeslice:
        controller:
          endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # EGS UI Chart Configuration
  kubeslice_ui_egs:
    release_name: "egs-ui"
    chart_ref: "./kubeslice-ui-egs"           # Using relative path
    release_namespace: "kubeslice-controller"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    release_values:
      global:
        imageRegistry: docker.io/aveshasystems
      kubeslice:
        prometheus:
          url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
        uiproxy:
          service:
            type: NodePort
          labels:
            app: kubeslice-ui-proxy
          annotations: {}
          ingress:
            enabled: false
            servicePort: 443
            className: ""
            hosts:
              - host: ui.kubeslice.com
                paths:
                  - path: /
                    pathType: Prefix
            tls: []
            annotations: []
            extraLabels: {}
        egsCoreApis:
          enabled: true
          service:
            type: ClusterIP
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # Kubeslice Worker Chart Configuration
  kubeslice_worker_egs_worker_1:
    release_name: "egs-worker"
    chart_ref: "./kubeslice-worker-egs"       # Using relative path
    release_namespace: "kubeslice-system"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "1.14.3"
    values_files:
      - "output/worker-1-values.yaml"
    release_values:
      cluster:
        name: "worker-1"
        endpoint: "https://{{ kubernetes_deployment.api_server.host }}:{{ kubernetes_deployment.api_server.port }}"
      global:
        imageRegistry: "docker.io/aveshasystems"
      egs:
        prometheusEndpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        grafanaDashboardBaseUrl: "http://<grafana-lb>/d/Oxed_c6Wz"
      metrics:
        insecure: true
      kserve:
        enabled: false
        kserve:
          controller:
            gateway:
              domain: "kubeslice.com"
              ingressGateway:
                className: "nginx"
      imagePullSecrets:
        registry: "{{ global_image_pull_secret.repository }}"
        username: "{{ global_image_pull_secret.username }}"
        password: "{{ global_image_pull_secret.password }}"

  # MetalLB Chart Configuration
  metallb_chart:
    release_name: "metallb"
    chart_ref: "metallb"
    release_namespace: "{{ metallb.namespace }}"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://metallb.github.io/metallb"
    chart_version: "0.13.10"
    release_values:
      prometheus:
        serviceMonitor:
          enabled: false
      controller:
        logLevel: info
      speaker:
        logLevel: info
        frr:
          enabled: false
      webhookConfiguration:
        enabled: true
        timeoutSeconds: 15
      crds:
        enabled: true
      rbac:
        create: true

  # Nginx Ingress Controller Chart
  nginx_ingress_chart:
    release_name: "ingress-nginx"
    chart_ref: "ingress-nginx"
    release_namespace: "ingress-nginx"
    create_namespace: true
    wait: true
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_repo_url: "https://kubernetes.github.io/ingress-nginx"
    chart_repo_name: "ingress-nginx"
    chart_version: "4.7.1"
    release_values:
      controller:
        replicaCount: 1
        service:
          enabled: true
          type: "NodePort"
        metrics:
          enabled: true
          serviceMonitor:
            enabled: false
        config:
          use-proxy-protocol: false
          real-ip-header: "proxy_protocol"
          proxy-real-ip-cidr: "0.0.0.0/0"
          use-forwarded-headers: "true"
        publishService:
          enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
            timeoutSeconds: 15
        resources:
          requests:
            cpu: "1000m"
            memory: "1024Mi"
          limits:
            cpu: "2000m"
            memory: "2048Mi"
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
      defaultBackend:
        enabled: false
        service:
          type: NodePort
        resources:
          requests:
            cpu: "1000m"
            memory: "2048"
          limits:
            cpu: "2000m"
            memory: "4096Mi"

  gpu_operator_chart:
    release_name: "gpu-operator"
    chart_ref: "gpu-operator"
    release_namespace: "gpu-operator"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_version: "v25.3.0"
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    dependencies: []
    release_values:
      mig:
        strategy: "none"
      dcgm:
        enabled: true
      driver:
        enabled: false
      cdi:
        default: true
        enabled: true
      hostPaths:
        driverInstallDir: "/home/kubernetes/bin/nvidia"
      toolkit:
        installDir: "/home/kubernetes/bin/nvidia"
        env:
          - name: "CONTAINERD_RUNTIME_CLASS"
            value: "nvidia"

  # AMD GPU Operator Chart
  amd_gpu_operator_chart:
    release_name: "amd-gpu-operator"
    chart_ref: "./amd-gpu-operator"
    release_namespace: "amd-gpu-operator"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []

  # Prometheus Stack
  prometheus_stack:
    release_name: "prometheus"
    chart_ref: "kube-prometheus-stack"
    release_namespace: "monitoring"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: false
    atomic: false
    skip_diff: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false
      prometheus:
        service:
          type: NodePort
        prometheusSpec:
          retention: "15d"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: local-path
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 10Gi
          additionalScrapeConfigs:
            - job_name: "gpu-metrics"
              scrape_interval: "1s"
              metrics_path: "/metrics"
              scheme: "http"
              kubernetes_sd_configs:
                - role: "endpoints"
                  namespaces:
                    names:
                      - "monitoring"
                      - "gpu-operator"
              relabel_configs:
                - source_labels: ["__meta_kubernetes_endpoints_name"]
                  action: "drop"
                  regex: ".*-node-feature-discovery-master"
                - source_labels: ["__meta_kubernetes_pod_node_name"]
                  action: "replace"
                  target_label: "kubernetes_node"
      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
      kubelet:
        serviceMonitor:
          https: false
      grafana:
        enabled: true
        persistence:
          enabled: true
          storageClassName: local-path
          size: 1Gi
          accessModes: ["ReadWriteOnce"]
        service:
          type: NodePort

  # KEDA Chart
  keda_chart:
    release_name: "keda"
    chart_ref: "keda/keda"    # Updated path to match actual chart location
    release_namespace: "keda"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    dependencies: []
    chart_version: "2.12.1"
    release_values: {}

  # NIM Operator Chart
  nim_operator_chart:
    release_name: "nim"
    chart_ref: "k8s-nim-operator"
    release_namespace: "nim"
    create_namespace: true
    wait: false
    timeout: "300s"
    force: true
    atomic: false
    skip_diff: true
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    chart_version: "v1.0.1"
    dependencies: []

###############################################################################
# KUBERNETES MANIFESTS CONFIGURATION
###############################################################################

manifests:

  # Local Path Retain StorageClass
  local_path_retain_storageclass:
    name: "local-path-retain-storageclass"
    manifest_file: "files/local-path-retain-storageclass.yaml.j2"
    namespace: ""  # StorageClass is cluster-scoped
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: []
    variables:
      storage_class_name: "local-path-retain"
      provisioner: "rancher.io/local-path"
      reclaim_policy: "Retain"
      volume_binding_mode: "WaitForFirstConsumer"
      allow_volume_expansion: true
      dir_mode: "0775"
      file_mode: "0664"
      mount_options:
        - "rw"
        - "exec"



  # EGS Project Configuration
  egs_project_manifest:
    name: "egs-project"
    manifest_file: "files/egs-project.yaml.j2"
    namespace: "kubeslice-controller"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - kubeslice_controller_egs
    variables:
      project_name: "avesha"
      project_namespace: "kubeslice-controller"
      admin_user: "admin"

  # EGS Cluster Registration Configuration
  egs_cluster_registration_worker_1:
    name: "egs-cluster-registration-worker-1"
    manifest_file: "files/egs-cluster-registration.yaml.j2"
    namespace: "kubeslice-avesha"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - egs_project_manifest
    variables:
      cluster_name: "worker-1"
      project_name: "avesha"
      telemetry:
        enabled: true
        endpoint: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
        telemetryProvider: "prometheus"
      geoLocation:
        cloudProvider: "DATACENTER"
        cloudRegion: ""


  # MetalLB IP Pool Configuration
  metallb_ip_pool:
    name: "metallb-ip-pool"
    manifest_file: "files/metallb-ip-pool.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      metallb_pool_name: "first-pool"
      metallb_namespace: "metallb"
      metallb_address_range: "172.18.1.1-172.18.1.16"  # Using the control plane IP

  # MetalLB L2 Advertisement Configuration
  metallb_l2_config:
    name: "metallb-l2-config"
    manifest_file: "files/metallb-l2-config.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: 
      - metallb_chart  # Ensure MetalLB chart is fully installed first
    variables:
      metallb_l2_name: "l2"
      metallb_namespace: "metallb"
      metallb_pool_name: "first-pool"

  # Nginx Ingress Additional Configuration
  nginx_ingress_config:
    name: "nginx-ingress-config"
    manifest_file: "files/nginx-ingress-config.yaml.j2"
    namespace: "ingress-nginx"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies: []
    variables:
      nginx_config_name: "nginx-ingress-controller-custom-config"
      nginx_namespace: "ingress-nginx"
      nginx_proxy_buffer_size: "128k"
      nginx_proxy_body_size: "50m"
      nginx_client_header_buffer_size: "64k"
      nginx_large_client_header_buffers: "4 128k"
      nginx_client_body_buffer_size: "128k"
      nginx_enable_brotli: "true"
      nginx_use_gzip: "true"
      nginx_gzip_level: "6"
      nginx_gzip_types: "application/atom+xml application/javascript application/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/javascript text/plain text/x-component"


  # AMD GPU DeviceConfig Manifest
  amd_gpu_deviceconfig_manifest:
    name: "amd-gpu-deviceconfig"
    manifest_file: "files/amd-gpu-deviceconfig.yaml.j2"
    namespace: "amd-gpu-operator"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - amd_gpu_operator_chart
    variables:
      deviceconfig_name: "amd-deviceconfig"
      deviceconfig_namespace: "amd-gpu-operator"
      driver_enable: false
      device_plugin_image: "rocm/k8s-device-plugin:latest"
      node_labeller_image: "rocm/k8s-device-plugin:labeller-latest"
      metrics_exporter_enable: true
      metrics_exporter_service_type: "NodePort"
      metrics_exporter_node_port: 32500
      metrics_exporter_image: "docker.io/rocm/device-metrics-exporter:v1.2.0"
      selector_label_key: "feature.node.kubernetes.io/amd-gpu"
      selector_label_value: "true"

  # OPTIONAL: Pushgateway Manifest
  pushgateway_manifest:
    name: "pushgateway-setup"
    manifest_file: "files/pushgateway.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    variables:
      pushgateway_name: "pushgateway"
      pushgateway_namespace: "monitoring"
      pushgateway_replicas: 1
      pushgateway_image: "prom/pushgateway"
      pushgateway_image_pull_policy: "IfNotPresent"
      pushgateway_service_name: "pushgateway"
      pushgateway_service_type: "ClusterIP"
      pushgateway_service_protocol: "TCP"
      pushgateway_service_port: 9091
      pushgateway_service_target_port: 9091
      pushgateway_monitor_name: "pushgateway"
      pushgateway_monitor_release: "prometheus"
      pushgateway_monitor_path: "/metrics"
      pushgateway_monitor_interval: "5s"
      pushgateway_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

 # NIM 1B Components
  nim_cache_manifest_1b:
    name: "nim-cache-setup-1b"
    manifest_file: "files/nim-cache-1b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-1b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.2-1b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_1b:
    name: "nim-service-setup-1b"
    manifest_file: "files/nim-service-1b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-1b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.2-1b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-1b-instruct"
      nim_service_storage_cache_profile: "" #TODO: change to 1b profile
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_1b:
    name: "keda-scaled-object-setup-1b"
    manifest_file: "files/keda-scaled-object-1b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-1b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-1b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: >-
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-1b-instruct"}

  smart_scaler_inference_1b:
    name: "smart-scaler-inference-setup-1b"
    manifest_file: "files/smart-scaler-inference-1b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-1b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-1b"
        cluster_name: "nim-llama-1b"
        tenant_id: "tenant-b200-local-1b"
        app_name: "nim-llama-1b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-1b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_1b:
    name: "locust-load-1b"
    manifest_file: "files/locust-deploy-1b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-1b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-1b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-1b"

 # NIM 1B Components
  nim_cache_manifest_8b:
    name: "nim-cache-setup-8b"
    manifest_file: "files/nim-cache-8b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-8b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.4"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "vllm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_8b:
    name: "nim-service-setup-8b"
    manifest_file: "files/nim-service-8b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-8b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.4"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-8b-instruct"
      nim_service_storage_cache_profile: "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_8b:
    name: "keda-scaled-object-setup-8b"
    manifest_file: "files/keda-scaled-object-8b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-8b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-8b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-8b-instruct"}

  smart_scaler_inference_8b:
    name: "smart-scaler-inference-setup-8b"
    manifest_file: "files/smart-scaler-inference-8b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-8b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-8b"
        cluster_name: "nim-llama-8b"
        tenant_id: "tenant-b200-local-8b"
        app_name: "nim-llama-8b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-8b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_8b:
    name: "locust-load-8b"
    manifest_file: "files/locust-deploy-8b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-8b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-8b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-8b"


   # OPTIONAL: NIM Cache Manifest
  nim_cache_manifest_70b:
    name: "nim-cache-setup-70b"
    manifest_file: "files/nim-cache-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_cache_name: "meta-llama3-70b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-70b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}


  nim_cache_wait_job_70b:
    name: "wait-for-nimcache-70b"
    manifest_file: "files/wait-job-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 1200
    wait_condition:
      type: "Succeeded"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      wait_job_name: "wait-for-nimcache-70b"
      wait_job_namespace: "nim"
      wait_job_target_cache: "meta-llama3-70b-instruct"
      wait_job_timeout: 1200
      wait_job_check_interval: 10
      wait_job_service_account: "nim-job-reader"

  nim_service_manifest_70b:
    name: "nim-service-setup-70b"
    manifest_file: "files/nim-service-70b.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    variables:
      nim_service_name: "meta-llama3-70b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-70b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-70b-instruct"
      nim_service_storage_cache_profile: "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_70b:
    name: "keda-scaled-object-setup-70b"
    manifest_file: "files/keda-scaled-object-70b.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      keda_scaled_object_name: "llm-demo-keda-70b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-70b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-70b-instruct"}

  smart_scaler_inference_70b:
    name: "smart-scaler-inference-setup-70b"
    manifest_file: "files/smart-scaler-inference-70b.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-70b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-70b"
        cluster_name: "nim-llama"
        tenant_id: "tenant-b200-local"
        app_name: "nim-llama"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-70b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_70b:
    name: "locust-load-70b"
    manifest_file: "files/locust-deploy-70b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      locust_name: "locust-load-70b"
      locust_namespace: "nim-load-test"
      locust_replicas: 0
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-70b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-70b"

  smart_scaler_mcp_server_manifest:
    name: "smart-scaler-mcp-server-setup"
    manifest_file: "files/smart-scaler-mcp-server.yaml.j2"
    namespace: "smart-scaler"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: false
    strict_validation: false
    variables:
      mcp_server_name: "smart-scaler-mcp-server"
      mcp_server_namespace: "smart-scaler"
      mcp_server_image: "docker.io/aveshasystems/smart-scaler-mcp-server:v1.0.1"
      mcp_server_image_pull_secret: "avesha-systems"
      mcp_server_prometheus_url: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090/"
      mcp_server_tenant_id: "tenant-b200-local"
      mcp_server_port: "8000"
      mcp_server_resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

  # NVIDIA Device Plugin DaemonSet
  nvidia_device_plugin_manifest:
    name: "nvidia-device-plugin-daemonset"
    manifest_file: "files/nvidia-device-plugin-daemonset.yaml.j2"
    namespace: "kube-system"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - gpu_operator_chart  # Ensure GPU operator is installed first
    variables:
      daemonset_name: "nvidia-device-plugin-daemonset"
      daemonset_namespace: "kube-system"
      device_plugin_image: "harbor.saas1.smart-scaler.io/avesha/aveshadev/k8s-device-plugin:v0.14.1"
      fail_on_init_error: "false"
      device_plugin_path: "/var/lib/kubelet/device-plugins"
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      priority_class_name: "system-node-critical"
      update_strategy:
        type: "RollingUpdate"
        rolling_update:
          max_unavailable: 1
          max_surge: 0
      revision_history_limit: 10

  # Jetson Stats Node Exporter DaemonSet
  jetson_stats_node_exporter_manifest:
    name: "jetson-stats-node-exporter"
    manifest_file: "files/jetson-stats-node-exporter.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - prometheus_stack  # Ensure monitoring namespace and Prometheus are available
      - jetson_prerequisites  # Ensure Jetson prerequisites are set up
    variables:
      daemonset_name: "jetson-stats-node-exporter"
      daemonset_namespace: "monitoring"
      exporter_image: "harbor.saas1.smart-scaler.io/avesha/magiccpp1/jetson_stats_node_exporter:v0.1.1"
      exporter_port: 9101
      python_path: "/usr/local/lib/python3.11/site-packages"
      resources:
        requests:
          cpu: "50m"
          memory: "64Mi"
        limits:
          cpu: "100m"
          memory: "128Mi"
      node_selector:
        kubernetes_io_arch: "arm64"
      tolerations:
        - key: "node-role.kubernetes.io/master"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      update_strategy:
        type: "RollingUpdate"
        rolling_update:
          max_unavailable: 1
          max_surge: 0
      revision_history_limit: 10

  # Jetson Stats Node Exporter Service
  jetson_stats_node_exporter_service_manifest:
    name: "jetson-stats-node-exporter-service"
    manifest_file: "files/jetson-stats-node-exporter-service.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - jetson_stats_node_exporter_manifest  # Ensure the DaemonSet is deployed first
    variables:
      service_name: "jetson-stats-node-exporter"
      service_namespace: "monitoring"
      service_port: 9101
      service_target_port: 9101
      service_type: "NodePort"
      prometheus_annotations:
        prometheus_io_path: "/metrics"
        prometheus_io_port: "9101"
        prometheus_io_scrape: "true"
      selector:
        app: "jetson-stats-node-exporter"

  # Jetson Stats Node Exporter ServiceMonitor
  jetson_stats_node_exporter_servicemonitor_manifest:
    name: "jetson-stats-node-exporter-servicemonitor"
    manifest_file: "files/jetson-stats-node-exporter-servicemonitor.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: false
    strict_validation: false
    dependencies:
      - jetson_stats_node_exporter_service_manifest  # Ensure the service is deployed first
      - prometheus_stack  # Ensure Prometheus operator is available
    variables:
      servicemonitor_name: "jetson-stats-node-exporter"
      servicemonitor_namespace: "monitoring"
      endpoint_interval: "30s"
      endpoint_path: "/metrics"
      endpoint_port: "metrics"
      endpoint_scrape_timeout: "10s"
      namespace_selector:
        match_names: ["monitoring"]
      selector:
        app: "jetson-stats-node-exporter"

###############################################################################
# COMMAND EXECUTION CONFIGURATION
###############################################################################

command_exec:
  - name: "create_cert_manager_issuers"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Creating cert-manager ClusterIssuer and Certificate resources..."
          cat <<EOF | kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: selfsigned-issuer
          spec:
            selfSigned: {}
          ---
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: my-selfsigned-ca
            namespace: cert-manager
          spec:
            isCA: true
            commonName: "avesha Labs Root CA"
            subject:
              organizationalUnits:
                - "IT Department"
              organizations:
                - "avesha Labs"
              countries:
                - "US"
            secretName: root-secret
            privateKey:
              algorithm: RSA
              size: 4096
            issuerRef:
              name: selfsigned-issuer
              kind: ClusterIssuer
              group: cert-manager.io
            duration: 8760h # 1 year
          ---
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: my-ca-issuer
          spec:
            ca:
              secretName: root-secret
          EOF
          echo "Cert-manager issuers and certificate created successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "create_smartai_namespaces_and_certificates"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Creating smartai namespaces..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smartai-backend --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smartai --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          echo "smartai namespaces created successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false
      - cmd: |
          echo "Creating smartai certificates..."
          cat <<EOF | kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: apps-dev-aveshalabs-io-tls
            namespace: smartai-backend
          spec:
            secretName: smartai-backend-tls
            issuerRef:
              name: my-ca-issuer
              kind: ClusterIssuer
              group: cert-manager.io
            commonName: smartai.avesha.lab
            dnsNames:
            - smartai.avesha.lab
            duration: 8760h # 1 year
            renewBefore: 720h # 30 days before expiry
            privateKey:
              algorithm: RSA
              size: 4096
          ---
          apiVersion: cert-manager.io/v1
          kind: Certificate
          metadata:
            name: apps-dev-aveshalabs-io-tls-smartai
            namespace: smartai
          spec:
            secretName: smartai-cluster-domain-tls-secret
            issuerRef:
              name: my-ca-issuer
              kind: ClusterIssuer
              group: cert-manager.io
            commonName: smartai.avesha.lab
            dnsNames:
            - smartai.avesha.lab
            duration: 8760h # 1 year
            renewBefore: 720h # 30 days before expiry
            privateKey:
              algorithm: RSA
              size: 4096
          EOF
          echo "smartai certificates created successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "verify_certificates_and_create_ca_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Extracting CA certificate and creating CA secrets..."
          # Extract CA certificate from root-secret
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret root-secret -n cert-manager -o jsonpath='{.data.tls\.crt}' | \
            base64 -d > avesha-labs-ca.crt
          
          # Create CA secret in smartai-backend namespace
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smartai-backend create secret generic smartai-ca-cert \
            --from-file=smartai-ca.pem=avesha-labs-ca.crt \
            --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          
          echo "CA certificate saved as: avesha-labs-ca.crt"
          echo "Distribute this file to clients that need to trust your certificates"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false
      - cmd: |
          echo "Waiting for certificates to be issued by cert-manager..."
          # Wait a bit for certificates to be issued
          sleep 15
          
          # Check if certificates are ready
          echo "Checking certificate status..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get certificate -n smartai-backend apps-dev-aveshalabs-io-tls -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Certificate status not available yet"
          
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get certificate -n smartai apps-dev-aveshalabs-io-tls-smartai -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Certificate status not available yet"
          
          echo "Certificate verification completed (OpenSSL not required)"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: true
      - cmd: |
          echo "Creating Run:AI CA secret with proper labels..."
          # Create the CA certificate secret in the smartai namespace
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smartai create secret generic smartai-ca-cert \
            --from-file=smartai-ca.pem=avesha-labs-ca.crt \
            --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          
          # Label the secret for Run:AI recognition
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            label secret smartai-ca-cert -n smartai \
            run.ai/cluster-wide=true run.ai/name=smartai-ca-cert --overwrite
          
          echo "Run:AI CA secrets created and labeled successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "apply_knative_serving_crds"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Applying Knative Serving CRDs using kubectl..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f files/serving-crds.yaml --timeout=30s
          
          echo "Waiting for CRDs to be established..."
          sleep 10
          
          echo "Knative Serving CRDs applied successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "apply_knative_serving_core"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Applying Knative Serving Core components using kubectl..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f files/serving-core.yaml --timeout=60s
          
          echo "Waiting for Knative Serving deployments to be ready..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Available deployment/activator \
            -n knative-serving --timeout=60s || true
          
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Available deployment/autoscaler \
            -n knative-serving --timeout=60s || true
          
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Available deployment/controller \
            -n knative-serving --timeout=60s || true
          
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Available deployment/webhook \
            -n knative-serving --timeout=60s || true
          
          echo "Knative Serving Core applied successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "apply_knative_serving_hpa"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Applying Knative Serving HPA components using kubectl..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f files/serving-hpa.yaml --timeout=30s
          
          echo "Waiting for HPA to be ready..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Ready hpa/activator \
            -n knative-serving --timeout=30s || true
          
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Ready hpa/webhook \
            -n knative-serving --timeout=30s || true
          
          echo "Knative Serving HPA applied successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "apply_knative_monitor"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Applying Knative Monitor components using kubectl..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f files/knative-monitor.yaml --timeout=60s
          
          echo "Waiting for Knative Monitor deployments to be ready..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            wait --for=condition=Available deployment \
            -l app.kubernetes.io/part-of=knative-serving \
            -n knative-serving --timeout=120s || true
          
          echo "Knative Monitor applied successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "patch_nodelocaldns_config"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Patching nodelocaldns configmap to add custom DNS entries..."
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            patch configmap -n kube-system nodelocaldns --patch '{
              "data": {
                "Corefile": "cluster.local:53 {\n    errors\n    cache {\n        success 9984 30\n        denial 9984 5\n    }\n    reload\n    loop\n    bind 169.254.25.10\n    forward . 10.233.0.3 {\n        force_tcp\n    }\n    prometheus :9253\n    health 169.254.25.10:9254\n}\nin-addr.arpa:53 {\n    errors\n    cache 30\n    reload\n    loop\n    bind 169.254.25.10\n    forward . 10.233.0.3 {\n        force_tcp\n    }\n    prometheus :9253\n}\nip6.arpa:53 {\n    errors\n    cache 30\n    reload\n    loop\n    bind 169.254.25.10\n    forward . 10.233.0.3 {\n        force_tcp\n    }\n    prometheus :9253\n}\n.:53 {\n    errors\n    cache 30\n    reload\n    loop\n    bind 169.254.25.10\n    hosts {\n        172.235.29.19 smartai.avesha.lab\n        fallthrough\n    }\n    forward . /etc/resolv.conf\n    prometheus :9253\n}"
              }
            }'
          echo "NodeLocalDNS configmap patched successfully!"
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: false

  - name: "wait_for_nim_cache_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          echo "Waiting for Job wait-for-nimcache-70b to complete..."
          start_time=$(date +%s)
          timeout=1200
          job_complete=false
          while [ $(($(date +%s) - start_time)) -lt $timeout ]; do
            if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
               --context={{ kubecontext | default(global_kubecontext) }} \
               -n nim get job wait-for-nimcache-70b -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' | grep -q "True"; then
              job_complete=true
              break
            fi
            echo "Job not complete yet, waiting... ($(($timeout - ($(date +%s) - start_time))) seconds remaining)"
            # Show job status
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim get job wait-for-nimcache-70b -o jsonpath='{.status.conditions[*]}' || true
            echo
            sleep 10
          done
          if [ "$job_complete" = true ]; then
            echo "Job completed successfully!"
          else
            echo "WARNING: Timeout waiting for Job"
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim describe job wait-for-nimcache-70b || true
          fi
          # Don't exit with error, let Ansible continue
        env:
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"
        ignore_errors: true
        
  - name: "create_ngc_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      # First check if secrets exist
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim get secret ngc-secret >/dev/null 2>&1; then
            echo "Secret ngc-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim delete secret ngc-secret
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim create secret docker-registry ngc-secret \
            --docker-server=nvcr.io \
            --docker-username='$oauthtoken' \
            --docker-password="${NGC_DOCKER_API_KEY}" \
            --docker-email='your.email@solo.io'
        env:
          NGC_DOCKER_API_KEY: "{{ ngc_docker_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

      # Handle ngc-api-secret
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret ngc-api-secret -n nim >/dev/null 2>&1; then
            echo "Secret ngc-api-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              delete secret ngc-api-secret -n nim
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create secret generic ngc-api-secret -n nim\
            --from-literal=NGC_API_KEY="${NGC_API_KEY}"
        env:
          NGC_API_KEY: "{{ ngc_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_ngc_secrets"
    commands:
      - cmd: "kubectl get secret ngc-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true
      - cmd: "kubectl get secret ngc-api-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi

          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true


  - name: "create_inference_pod_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-1b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-1b --from-file="config.json"="files/config-inference-1b.json"

  - name: "create_inference_pod_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-8b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-8b --from-file="config.json"="files/config-inference-8b.json"

  - name: "create_inference_pod_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-70b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-70b --from-file="config.json"="files/config-inference-70b.json"

  - name: "create_locust_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-1b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-1b --from-file="locustfile.py"="files/locust-1b.py"

  - name: "create_locust_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-8b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-8b --from-file="locustfile.py"="files/locust-8b.py"

  - name: "create_locust_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-70b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-70b --from-file="locustfile.py"="files/locust-70b.py"

  - name: "fetch_worker_secret_worker_1"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # Create output directory if it doesn't exist
          mkdir -p output
          
          # Extract and encode values from the secret to create the values file
          echo "Generating worker values file..."
          echo "controllerSecret:" > output/worker-1-values.yaml
          
          # Extract namespace
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.namespace}' | \
            echo "  namespace: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract endpoint
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.controllerEndpoint}' | \
            echo "  endpoint: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract ca.crt
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.ca\.crt}' | \
            echo "  ca.crt: $(cat -)" >> output/worker-1-values.yaml
          
          # Extract token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-worker-worker-1 -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            echo "  token: $(cat -)" >> output/worker-1-values.yaml
          
          echo "" >> output/worker-1-values.yaml
          echo "egsAgent:" >> output/worker-1-values.yaml
          echo "  secretName: egs-agent-access" >> output/worker-1-values.yaml
          echo "  agentSecret:" >> output/worker-1-values.yaml
          
          # Get UI proxy service endpoint
          UI_PROXY_PORT=$(kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get svc kubeslice-ui-proxy -n kubeslice-controller -o jsonpath='{.spec.ports[0].port}')
          
          echo "    endpoint: http://kubeslice-ui-proxy.kubeslice-controller.svc.cluster.local:${UI_PROXY_PORT}" >> output/worker-1-values.yaml
          
          # Get and decode the admin token
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret kubeslice-rbac-rw-admin -n kubeslice-avesha -o jsonpath='{.data.token}' | \
            base64 -d | \
            echo "    key: $(cat -)" >> output/worker-1-values.yaml
          
          echo "Worker values file generated at output/worker-1-values.yaml"
